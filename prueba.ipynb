{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_code import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process file c:\\Users\\agianolini\\OneDrive - ANDES WEALTH MANAGEMENT SA\\Desktop\\research-assistant-main\\nlp_prueba\\prueba.ipynb: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nlp_prueba': {'.git': {'hooks': {},\n",
       "   'info': {},\n",
       "   'logs': {'refs': {'heads': {}, 'remotes': {'origin': {}}}},\n",
       "   'objects': {'20': {},\n",
       "    '25': {},\n",
       "    '64': {},\n",
       "    '90': {},\n",
       "    '97': {},\n",
       "    'c2': {},\n",
       "    'c4': {},\n",
       "    'e5': {},\n",
       "    'f8': {},\n",
       "    'info': {},\n",
       "    'pack': {}},\n",
       "   'refs': {'heads': {}, 'remotes': {'origin': {}}, 'tags': {}}},\n",
       "  '__pycache__': {},\n",
       "  'code_search.py': {'functions': {'__init__': {'code': '    def __init__(self, model_path, code_snippets, device=None):\\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\\n        self.tokenizer = RobertaTokenizer.from_pretrained(\\'microsoft/unixcoder-base\\')\\n        self.model = self.load_model(model_path)\\n        self.code_snippets = code_snippets\\n        self.code_embeddings = self.generate_code_embeddings(code_snippets)',\n",
       "     'lines': {'start': 22, 'end': 28},\n",
       "     'docstring': None},\n",
       "    'forward': {'code': '    def forward(self, code_inputs=None, nl_inputs=None):\\n        if code_inputs is not None:\\n            outputs = self.encoder(code_inputs,attention_mask=code_inputs.ne(1))[0]\\n            outputs = (outputs*code_inputs.ne(1)[:,:,None]).sum(1)/code_inputs.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)\\n        else:\\n            outputs = self.encoder(nl_inputs,attention_mask=nl_inputs.ne(1))[0]\\n            outputs = (outputs*nl_inputs.ne(1)[:,:,None]).sum(1)/nl_inputs.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)',\n",
       "     'lines': {'start': 11, 'end': 20},\n",
       "     'docstring': None},\n",
       "    'load_model': {'code': \"    def load_model(self, model_path):\\n        model = RobertaModel.from_pretrained('microsoft/unixcoder-base')\\n        model = Model(model)\\n        model.load_state_dict(torch.load(model_path, map_location=self.device))\\n        model.to(self.device)\\n        model.eval()\\n        return model\",\n",
       "     'lines': {'start': 29, 'end': 36},\n",
       "     'docstring': None},\n",
       "    'generate_code_embeddings': {'code': \"    def generate_code_embeddings(self, code_snippets):\\n        embeddings = []\\n        for snippet in code_snippets:\\n            inputs = self.tokenizer.encode_plus(snippet, add_special_tokens=True, max_length=256, truncation=True, padding='max_length', return_tensors='pt')\\n            with torch.no_grad():\\n                embedding = self.model(code_inputs=inputs['input_ids'].to(self.device))\\n            embeddings.append(embedding.cpu().numpy())\\n        return np.vstack(embeddings)\",\n",
       "     'lines': {'start': 37, 'end': 45},\n",
       "     'docstring': None},\n",
       "    'get_query_embedding': {'code': \"    def get_query_embedding(self, query):\\n        inputs = self.tokenizer.encode_plus(query, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\\n        with torch.no_grad():\\n            embedding = self.model(code_inputs=inputs['input_ids'].to(self.device))\\n        return embedding.cpu().numpy()\",\n",
       "     'lines': {'start': 46, 'end': 51},\n",
       "     'docstring': None},\n",
       "    'get_similarity_search': {'code': \"    def get_similarity_search(self, query, k):\\n        query_embedding = self.get_query_embedding(query)\\n        similarities = 1 - cdist(query_embedding, self.code_embeddings, 'cosine').flatten()\\n\\n        # Get top-k indices\\n        top_k_indices = np.argsort(similarities)[-k:]\\n\\n        # Create a dictionary for the top k results\\n        results = {}\\n        for index in reversed(top_k_indices):\\n            snippet_info = {\\n                'index': index,\\n                'snippet': self.code_snippets[index],\\n                'similarity': similarities[index]\\n            }\\n            results[f'top_{k}'] = snippet_info\\n            k -= 1\\n    \\n        return results\",\n",
       "     'lines': {'start': 52, 'end': 71},\n",
       "     'docstring': None}},\n",
       "   'classes': {'Model': {'code': 'class Model(nn.Module):\\n    def __init__(self, encoder):\\n        super(Model, self).__init__()\\n        self.encoder = encoder\\n\\n    def forward(self, code_inputs=None, nl_inputs=None):\\n        if code_inputs is not None:\\n            outputs = self.encoder(code_inputs,attention_mask=code_inputs.ne(1))[0]\\n            outputs = (outputs*code_inputs.ne(1)[:,:,None]).sum(1)/code_inputs.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)\\n        else:\\n            outputs = self.encoder(nl_inputs,attention_mask=nl_inputs.ne(1))[0]\\n            outputs = (outputs*nl_inputs.ne(1)[:,:,None]).sum(1)/nl_inputs.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)',\n",
       "     'lines': {'start': 6, 'end': 20},\n",
       "     'docstring': None},\n",
       "    'CodeSearcher': {'code': 'class CodeSearcher:\\n    def __init__(self, model_path, code_snippets, device=None):\\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\\n        self.tokenizer = RobertaTokenizer.from_pretrained(\\'microsoft/unixcoder-base\\')\\n        self.model = self.load_model(model_path)\\n        self.code_snippets = code_snippets\\n        self.code_embeddings = self.generate_code_embeddings(code_snippets)\\n\\n    def load_model(self, model_path):\\n        model = RobertaModel.from_pretrained(\\'microsoft/unixcoder-base\\')\\n        model = Model(model)\\n        model.load_state_dict(torch.load(model_path, map_location=self.device))\\n        model.to(self.device)\\n        model.eval()\\n        return model\\n\\n    def generate_code_embeddings(self, code_snippets):\\n        embeddings = []\\n        for snippet in code_snippets:\\n            inputs = self.tokenizer.encode_plus(snippet, add_special_tokens=True, max_length=256, truncation=True, padding=\\'max_length\\', return_tensors=\\'pt\\')\\n            with torch.no_grad():\\n                embedding = self.model(code_inputs=inputs[\\'input_ids\\'].to(self.device))\\n            embeddings.append(embedding.cpu().numpy())\\n        return np.vstack(embeddings)\\n\\n    def get_query_embedding(self, query):\\n        inputs = self.tokenizer.encode_plus(query, add_special_tokens=True, max_length=128, truncation=True, padding=\\'max_length\\', return_tensors=\\'pt\\')\\n        with torch.no_grad():\\n            embedding = self.model(code_inputs=inputs[\\'input_ids\\'].to(self.device))\\n        return embedding.cpu().numpy()\\n\\n    def get_similarity_search(self, query, k):\\n        query_embedding = self.get_query_embedding(query)\\n        similarities = 1 - cdist(query_embedding, self.code_embeddings, \\'cosine\\').flatten()\\n\\n        # Get top-k indices\\n        top_k_indices = np.argsort(similarities)[-k:]\\n\\n        # Create a dictionary for the top k results\\n        results = {}\\n        for index in reversed(top_k_indices):\\n            snippet_info = {\\n                \\'index\\': index,\\n                \\'snippet\\': self.code_snippets[index],\\n                \\'similarity\\': similarities[index]\\n            }\\n            results[f\\'top_{k}\\'] = snippet_info\\n            k -= 1\\n    \\n        return results',\n",
       "     'lines': {'start': 21, 'end': 71},\n",
       "     'docstring': None}},\n",
       "   'direct_imports': {'torch.nn': {'alias': 'nn', 'lineno': 1},\n",
       "    'torch': {'alias': None, 'lineno': 2},\n",
       "    'numpy': {'alias': 'np', 'lineno': 4}},\n",
       "   'from_imports': {'transformers': [{'name': 'RobertaTokenizer',\n",
       "      'alias': None,\n",
       "      'lineno': 3},\n",
       "     {'name': 'RobertaModel', 'alias': None, 'lineno': 3}],\n",
       "    'scipy.spatial.distance': [{'name': 'cdist', 'alias': None, 'lineno': 5}]},\n",
       "   'module_docstring': None},\n",
       "  'code_searcher.py': {'functions': {'__init__': {'code': '    def __init__(self, model_path, code_snippets, device=None):\\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\\n        self.tokenizer = RobertaTokenizer.from_pretrained(\\'microsoft/unixcoder-base\\')\\n        self.model = self.load_model(model_path)\\n        self.code_snippets = code_snippets\\n        self.code_embeddings = self.generate_code_embeddings(code_snippets)',\n",
       "     'lines': {'start': 29, 'end': 35},\n",
       "     'docstring': None},\n",
       "    'forward': {'code': '    def forward(self, code_inputs=None, nl_inputs=None):\\n        if code_inputs is not None:\\n            outputs = self.encoder(code_inputs,attention_mask=code_inputs.ne(1))[0]\\n            outputs = (outputs*code_inputs.ne(1)[:,:,None]).sum(1)/code_inputs.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)\\n        else:\\n            outputs = self.encoder(nl_inputs,attention_mask=nl_inputs.ne(1))[0]\\n            outputs = (outputs*nl_inputs.ne(1)[:,:,None]).sum(1)/nl_inputs.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)',\n",
       "     'lines': {'start': 17, 'end': 26},\n",
       "     'docstring': None},\n",
       "    'load_model': {'code': \"    def load_model(self, model_path):\\n        model = RobertaModel.from_pretrained('microsoft/unixcoder-base')\\n        model = Model(model)\\n        model.load_state_dict(torch.load(model_path, map_location=self.device))\\n        model.to(self.device)\\n        model.eval()\\n        return model\",\n",
       "     'lines': {'start': 36, 'end': 43},\n",
       "     'docstring': None},\n",
       "    'generate_code_embeddings': {'code': \"    def generate_code_embeddings(self, code_snippets):\\n        embeddings = []\\n        for snippet in code_snippets:\\n            inputs = self.tokenizer.encode_plus(snippet, add_special_tokens=True, max_length=256, truncation=True, padding='max_length', return_tensors='pt')\\n            with torch.no_grad():\\n                embedding = self.model(code_inputs=inputs['input_ids'].to(self.device))\\n            embeddings.append(embedding.cpu().numpy())\\n        return np.vstack(embeddings)\",\n",
       "     'lines': {'start': 44, 'end': 52},\n",
       "     'docstring': None},\n",
       "    'get_query_embedding': {'code': \"    def get_query_embedding(self, query):\\n        inputs = self.tokenizer.encode_plus(query, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\\n        with torch.no_grad():\\n            embedding = self.model(code_inputs=inputs['input_ids'].to(self.device))\\n        return embedding.cpu().numpy()\",\n",
       "     'lines': {'start': 53, 'end': 58},\n",
       "     'docstring': None},\n",
       "    'get_similarity_search': {'code': \"    def get_similarity_search(self, query, k):\\n        query_embedding = self.get_query_embedding(query)\\n        similarities = 1 - cdist(query_embedding, self.code_embeddings, 'cosine').flatten()\\n\\n        # Get top-k indices\\n        top_k_indices = np.argsort(similarities)[-k:]\\n\\n        # Create a dictionary for the top k results\\n        results = {}\\n        for index in reversed(top_k_indices):\\n            snippet_info = {\\n                'index': index,\\n                'snippet': self.code_snippets[index],\\n                'similarity': similarities[index]\\n            }\\n            results[f'top_{k}'] = snippet_info\\n            k -= 1\\n\\n        return results\",\n",
       "     'lines': {'start': 59, 'end': 78},\n",
       "     'docstring': None}},\n",
       "   'classes': {'Model': {'code': 'class Model(nn.Module):\\n    def __init__(self, encoder):\\n        super(Model, self).__init__()\\n        self.encoder = encoder\\n\\n    def forward(self, code_inputs=None, nl_inputs=None):\\n        if code_inputs is not None:\\n            outputs = self.encoder(code_inputs,attention_mask=code_inputs.ne(1))[0]\\n            outputs = (outputs*code_inputs.ne(1)[:,:,None]).sum(1)/code_inputs.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)\\n        else:\\n            outputs = self.encoder(nl_inputs,attention_mask=nl_inputs.ne(1))[0]\\n            outputs = (outputs*nl_inputs.ne(1)[:,:,None]).sum(1)/nl_inputs.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)',\n",
       "     'lines': {'start': 12, 'end': 26},\n",
       "     'docstring': None},\n",
       "    'CodeSearcher': {'code': 'class CodeSearcher:\\n    def __init__(self, model_path, code_snippets, device=None):\\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\\n        self.tokenizer = RobertaTokenizer.from_pretrained(\\'microsoft/unixcoder-base\\')\\n        self.model = self.load_model(model_path)\\n        self.code_snippets = code_snippets\\n        self.code_embeddings = self.generate_code_embeddings(code_snippets)\\n\\n    def load_model(self, model_path):\\n        model = RobertaModel.from_pretrained(\\'microsoft/unixcoder-base\\')\\n        model = Model(model)\\n        model.load_state_dict(torch.load(model_path, map_location=self.device))\\n        model.to(self.device)\\n        model.eval()\\n        return model\\n\\n    def generate_code_embeddings(self, code_snippets):\\n        embeddings = []\\n        for snippet in code_snippets:\\n            inputs = self.tokenizer.encode_plus(snippet, add_special_tokens=True, max_length=256, truncation=True, padding=\\'max_length\\', return_tensors=\\'pt\\')\\n            with torch.no_grad():\\n                embedding = self.model(code_inputs=inputs[\\'input_ids\\'].to(self.device))\\n            embeddings.append(embedding.cpu().numpy())\\n        return np.vstack(embeddings)\\n\\n    def get_query_embedding(self, query):\\n        inputs = self.tokenizer.encode_plus(query, add_special_tokens=True, max_length=128, truncation=True, padding=\\'max_length\\', return_tensors=\\'pt\\')\\n        with torch.no_grad():\\n            embedding = self.model(code_inputs=inputs[\\'input_ids\\'].to(self.device))\\n        return embedding.cpu().numpy()\\n\\n    def get_similarity_search(self, query, k):\\n        query_embedding = self.get_query_embedding(query)\\n        similarities = 1 - cdist(query_embedding, self.code_embeddings, \\'cosine\\').flatten()\\n\\n        # Get top-k indices\\n        top_k_indices = np.argsort(similarities)[-k:]\\n\\n        # Create a dictionary for the top k results\\n        results = {}\\n        for index in reversed(top_k_indices):\\n            snippet_info = {\\n                \\'index\\': index,\\n                \\'snippet\\': self.code_snippets[index],\\n                \\'similarity\\': similarities[index]\\n            }\\n            results[f\\'top_{k}\\'] = snippet_info\\n            k -= 1\\n\\n        return results',\n",
       "     'lines': {'start': 28, 'end': 78},\n",
       "     'docstring': None}},\n",
       "   'direct_imports': {'gdown': {'alias': None, 'lineno': 1},\n",
       "    'os': {'alias': None, 'lineno': 2},\n",
       "    'torch.nn': {'alias': 'nn', 'lineno': 3},\n",
       "    'torch': {'alias': None, 'lineno': 4},\n",
       "    'numpy': {'alias': 'np', 'lineno': 6}},\n",
       "   'from_imports': {'transformers': [{'name': 'RobertaTokenizer',\n",
       "      'alias': None,\n",
       "      'lineno': 5},\n",
       "     {'name': 'RobertaModel', 'alias': None, 'lineno': 5}],\n",
       "    'scipy.spatial.distance': [{'name': 'cdist', 'alias': None, 'lineno': 7}]},\n",
       "   'module_docstring': None},\n",
       "  'extract_code.py': {'functions': {'code_to_json': {'code': 'def code_to_json(code_lines, tree):\\n    res = {}\\n    functions = {}\\n    classes = {}\\n    direct_imports = {}\\n    from_imports = defaultdict(list)\\n\\n    # Extract module-level docstring, if present\\n    module_docstring = ast.get_docstring(tree)\\n\\n    for node in ast.walk(tree):\\n        if isinstance(node, ast.FunctionDef):\\n            function_code = \"\\\\n\".join(code_lines[node.lineno - 1: node.end_lineno])\\n            function_lines = {\"start\": node.lineno - 1, \"end\": node.end_lineno}\\n            function_docstring = ast.get_docstring(node)\\n            functions[node.name] = {\"code\": function_code, \"lines\": function_lines, \"docstring\": function_docstring}\\n        elif isinstance(node, ast.ClassDef):\\n            class_code = \"\\\\n\".join(code_lines[node.lineno - 1: node.end_lineno])\\n            class_lines = {\"start\": node.lineno - 1, \"end\": node.end_lineno}\\n            class_docstring = ast.get_docstring(node)\\n            classes[node.name] = {\"code\": class_code, \"lines\": class_lines, \"docstring\": class_docstring}\\n        elif isinstance(node, ast.Import):\\n            for alias in node.names:\\n                direct_imports[alias.name] = {\"alias\": alias.asname, \"lineno\": node.lineno}\\n        elif isinstance(node, ast.ImportFrom):\\n            for alias in node.names:\\n                from_imports[node.module].append({\\n                    \"name\": alias.name,\\n                    \"alias\": alias.asname,\\n                    \"lineno\": node.lineno\\n                })\\n\\n    res[\"functions\"] = functions\\n    res[\"classes\"] = classes\\n    res[\"direct_imports\"] = direct_imports\\n    res[\"from_imports\"] = dict(from_imports)  # Convert back to a regular dict for JSON serialization\\n    res[\"module_docstring\"] = module_docstring\\n\\n    return res',\n",
       "     'lines': {'start': 7, 'end': 46},\n",
       "     'docstring': None},\n",
       "    'get_file_code': {'code': \"def get_file_code(file_path):\\n    with open(file_path, 'r', encoding='utf-8') as f:\\n        if file_path.endswith('.py'):\\n            code = f.read()\\n            code_lines = code.split('\\\\n')\\n            tree = ast.parse(code)\\n            res = code_to_json(code_lines, tree)\\n        elif file_path.endswith('.ipynb'):\\n            notebook = json.load(f)\\n            res = {'cells': [cell['source'] for cell in notebook['cells'] if cell['cell_type'] == 'code']}\\n    return res\",\n",
       "     'lines': {'start': 48, 'end': 59},\n",
       "     'docstring': None},\n",
       "    'parse_md': {'code': \"def parse_md(file_path):\\n    with open(file_path, 'r', encoding='utf-8') as f:\\n        return markdown.markdown(f.read())\",\n",
       "     'lines': {'start': 61, 'end': 64},\n",
       "     'docstring': None},\n",
       "    'get_project_structure': {'code': 'def get_project_structure(project_path):\\n    res = {}\\n    file_path = None\\n\\n    project_name = os.path.basename(project_path.rstrip(os.sep)) # Get the project folder name\\n\\n    for root, dirs, files in os.walk(project_path):\\n        # Skip if it\\'s before the project directory\\n        if project_name not in root:\\n            continue\\n\\n        path_parts = root.split(os.sep)\\n        # Start from the project directory in the path_parts\\n        start_index = path_parts.index(project_name)\\n        relevant_path_parts = path_parts[start_index:]\\n\\n        current_level = res\\n        for part in relevant_path_parts:\\n            if part not in current_level:\\n                current_level[part] = {}\\n            current_level = current_level[part]\\n        structure = current_level\\n\\n        for directory in dirs:\\n            structure[directory] = {}\\n\\n        for file in files:\\n            try:\\n                file_path = os.path.join(root, file)\\n                if file.endswith(\".py\") or file.endswith(\".ipynb\"):\\n                    structure[file] = get_file_code(file_path)\\n                elif file.endswith(\".md\"):\\n                    structure[file] = parse_md(file_path)\\n            except Exception as e:\\n                print(f\"Failed to process file {file_path}: {e}\")\\n\\n    return res',\n",
       "     'lines': {'start': 66, 'end': 103},\n",
       "     'docstring': None},\n",
       "    'summarize_structure': {'code': \"def summarize_structure(project_structure, indent=''):\\n    python_files = 0\\n    ipynb_files = 0\\n    md_files = 0\\n    function_counts = []\\n    class_counts = []\\n    import_counts = []\\n\\n    for key, value in project_structure.items():\\n        if isinstance(value, dict):\\n            if key.endswith('.py') or key.endswith('.ipynb'):\\n                python_files += 1\\n                function_counts.append(len(value.get('functions', {})))\\n                class_counts.append(len(value.get('classes', {})))\\n                import_counts.append(len(value.get('imports', {})))\\n            elif key.endswith('.md'):\\n                md_files += 1\\n            else:\\n                # recurse into subdirectories\\n                sub_python_files, sub_ipynb_files, sub_md_files, sub_function_counts, sub_class_counts, sub_import_counts = summarize_structure(\\n                    value, indent + '  ')\\n                python_files += sub_python_files\\n                ipynb_files += sub_ipynb_files\\n                md_files += sub_md_files\\n                function_counts.extend(sub_function_counts)\\n                class_counts.extend(sub_class_counts)\\n                import_counts.extend(sub_import_counts)\\n\\n    return python_files, ipynb_files, md_files, function_counts, class_counts, import_counts\",\n",
       "     'lines': {'start': 104, 'end': 133},\n",
       "     'docstring': None},\n",
       "    'get_imports': {'code': 'def get_imports(tree):\\n    imports = set()\\n\\n    for node in ast.walk(tree):\\n        if isinstance(node, ast.Import):\\n            for alias in node.names:\\n                imports.add(alias.name)\\n        elif isinstance(node, ast.ImportFrom):\\n            if node.level > 0:\\n                # We have a relative import, which we\\'ll ignore for now\\n                continue\\n\\n            module = node.module\\n            for alias in node.names:\\n                if module is None:\\n                    imports.add(alias.name)\\n                else:\\n                    imports.add(f\"{module}.{alias.name}\")\\n\\n    return imports',\n",
       "     'lines': {'start': 135, 'end': 155},\n",
       "     'docstring': None},\n",
       "    'analyze_codebase': {'code': \"def analyze_codebase(directory):\\n    results = {}\\n\\n    # Step 2-3: Traverse directory and parse each Python file\\n    for file_path in traverse_directory(directory):\\n        with open(file_path, 'r') as f:\\n            code = f.read()\\n        tree = ast.parse(code)\\n        code_lines = code.splitlines()\\n        results[file_path] = code_to_json(code_lines, tree)\\n\\n    # Step 4-5: Analyze imports to find dependencies\\n    dependencies = analyze_dependencies(results)\\n\\n    return results, dependencies\",\n",
       "     'lines': {'start': 157, 'end': 172},\n",
       "     'docstring': None},\n",
       "    'traverse_directory': {'code': 'def traverse_directory(directory):\\n    for root, dirs, files in os.walk(directory):\\n        for file in files:\\n            if file.endswith(\".py\"):\\n                yield os.path.join(root, file)',\n",
       "     'lines': {'start': 177, 'end': 182},\n",
       "     'docstring': None},\n",
       "    'is_external': {'code': \"def is_external(module):\\n    try:\\n        spec = importlib.util.find_spec(module)\\n        return spec is not None and 'site-packages' in spec.origin\\n    except ModuleNotFoundError:\\n        return False\",\n",
       "     'lines': {'start': 184, 'end': 190},\n",
       "     'docstring': None},\n",
       "    'analyze_dependencies': {'code': \"def analyze_dependencies(results):\\n    dependencies = {file: set() for file in results.keys()}\\n\\n    for file, structure in results.items():\\n        if 'direct_imports' in structure:\\n            for imported_module in structure['direct_imports']:\\n                if not is_external(imported_module):\\n                    # Note: This code assumes the imported file is in the same directory\\n                    # This might not be the case in your project.\\n                    dependencies[file].add(imported_module)\\n\\n    return dependencies\",\n",
       "     'lines': {'start': 192, 'end': 204},\n",
       "     'docstring': None},\n",
       "    'process_atlas_data': {'code': 'def process_atlas_data(atlas_data):\\n    # Placeholder for the documents\\n    documents = []\\n\\n    # Iterate over the files in the atlas data\\n    for file_path, file_data in atlas_data.items():\\n        # Check if the file data is a dictionary\\n        if isinstance(file_data, dict):\\n            # Process the functions\\n            for function_name, function_data in file_data.get(\\'functions\\', {}).items():\\n                # Construct the document ID\\n                document_id = f\"{file_path}/function/{function_name}\"\\n\\n                # Prepare the document content and metadata\\n                document_content = json.dumps(function_data)\\n                document_metadata = {\\'file_path\\': file_path, \\'type\\': \\'function\\', \\'name\\': function_name}\\n\\n                # Add the document to the list\\n                documents.append((document_id, document_content, document_metadata))\\n\\n            # Process the classes\\n            for class_name, class_data in file_data.get(\\'classes\\', {}).items():\\n                # Construct the document ID\\n                document_id = f\"{file_path}/class/{class_name}\"\\n\\n                # Prepare the document content and metadata\\n                document_content = json.dumps(class_data)\\n                document_metadata = {\\'file_path\\': file_path, \\'type\\': \\'class\\', \\'name\\': class_name}\\n\\n                # Add the document to the list\\n                documents.append((document_id, document_content, document_metadata))\\n\\n    return documents',\n",
       "     'lines': {'start': 206, 'end': 239},\n",
       "     'docstring': None}},\n",
       "   'classes': {},\n",
       "   'direct_imports': {'ast': {'alias': None, 'lineno': 1},\n",
       "    'markdown': {'alias': None, 'lineno': 2},\n",
       "    'json': {'alias': None, 'lineno': 3},\n",
       "    'importlib.util': {'alias': None, 'lineno': 5},\n",
       "    'os': {'alias': None, 'lineno': 175}},\n",
       "   'from_imports': {'collections': [{'name': 'defaultdict',\n",
       "      'alias': None,\n",
       "      'lineno': 4}]},\n",
       "   'module_docstring': None},\n",
       "  'main copy.py': {'functions': {'get_project_structure': {'code': 'def get_project_structure(project_path):\\n    res = {}\\n    repo_name = os.path.basename(project_path)\\n\\n    for root, dirs, files in os.walk(project_path):\\n        # Trim the root to start from the repository\\'s root directory\\n        trimmed_root = root[len(project_path):].lstrip(os.sep)\\n        path_parts = trimmed_root.split(os.sep)\\n        current_level = res\\n\\n        # Construct the nested dictionary structure\\n        for part in path_parts:\\n            current_level = current_level.setdefault(part, {})\\n\\n        for file in files:\\n            try:\\n                file_path = os.path.join(root, file)\\n                if file.endswith(\".py\") or file.endswith(\".ipynb\"):\\n                    current_level[file] = get_file_code(file_path)\\n                elif file.endswith(\".md\"):\\n                    current_level[file] = parse_md(file_path)\\n            except Exception as e:\\n                print(f\"Failed to process file {file_path}: {e}\")\\n\\n    # Wrap in the repository\\'s name\\n    return {repo_name: res}',\n",
       "     'lines': {'start': 10, 'end': 36},\n",
       "     'docstring': None},\n",
       "    'clone_github_repo': {'code': 'def clone_github_repo(github_url, dest_folder):\\n    \"\"\"\\n    Clone a GitHub repository to the specified local directory, even if the directory is not empty.\\n    \"\"\"\\n    try:\\n        # Create a temporary directory to clone the repo\\n        with tempfile.TemporaryDirectory() as temp_dir:\\n            # Clone the repository into the temporary directory\\n            subprocess.run([\"git\", \"clone\", github_url, temp_dir], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\\n            \\n            # Move the contents of the cloned repo to the destination folder\\n            repo_name = os.path.basename(github_url.rstrip(\\'/\\').split(\\'/\\')[-1])\\n            source_folder = os.path.join(temp_dir, repo_name)\\n            for item in os.listdir(source_folder):\\n                s = os.path.join(source_folder, item)\\n                d = os.path.join(dest_folder, item)\\n                if os.path.isdir(s):\\n                    shutil.copytree(s, d, dirs_exist_ok=True)\\n                else:\\n                    shutil.copy2(s, d)\\n            \\n            print(\"Repository cloned successfully.\")\\n\\n    except subprocess.CalledProcessError as e:\\n        print(f\"Error in subprocess: {e.stderr.decode().strip()}\")\\n        raise\\n    except Exception as e:\\n        print(f\"Error: {e}\")\\n        raise',\n",
       "     'lines': {'start': 36, 'end': 65},\n",
       "     'docstring': 'Clone a GitHub repository to the specified local directory, even if the directory is not empty.'},\n",
       "    'main': {'code': 'def main():\\n    parser = argparse.ArgumentParser(description=\\'Parse a project structure into a JSON representation.\\')\\n    # parser.add_argument(\\'--github_url\\', type=str, required=True, help=\\'URL to the GitHub repository.\\')    \\n    parser.add_argument(\\'--github_url\\', type=str, default= \"https://github.com/dr-aheydari/SoftAdapt\", help=\\'URL to the GitHub repository.\\')    \\n    # parser.add_argument(\\'--pdf_path\\', type=str, required=True, help=\\'Path to the paper PDF\\')\\n    # parser.add_argument(\\'--output_dir_path\\', type=str, required=True, help=\\'Path to the output dir.\\')\\n    parser.add_argument(\\'--output_dir_path\\', type=str, default= r\"C:\\\\Users\\\\agianolini\\\\OneDrive - ANDES WEALTH MANAGEMENT SA\\\\Desktop\\\\research-assistant-main\", \\n                        help=\\'Path to the output dir.\\')\\n    parser.add_argument(\\'--model_path\\', type=str, default=r\"C:\\\\Users\\\\agianolini\\\\OneDrive - ANDES WEALTH MANAGEMENT SA\\\\Desktop\\\\research-assistant-main\\\\model (7).bin\") # change to your model path\\n    parser.add_argument(\\'--nl_query\\', type=str, default=\\'a for loop that prints the world\\')\\n    args = parser.parse_args()\\n    \\n    temp_dir = r\"C:\\\\Users\\\\agianolini\\\\OneDrive - ANDES WEALTH MANAGEMENT SA\\\\Desktop\\\\research-assistant-main\\\\output\" # Temporary directory to clone the repo\\n    clone_github_repo(args.github_url, temp_dir)\\n\\n    # Get the project structure\\n    project_structure = get_project_structure(temp_dir)\\n\\n    # Save the project structure to a JSON file\\n    os.makedirs(args.output_dir_path, exist_ok=True)\\n    project_name = os.path.basename(args.github_url)\\n    output_path = os.path.join(args.output_dir_path, f\\'{project_name}.json\\')\\n    with open(output_path, \\'w\\', encoding=\\'utf-8\\') as f:\\n        json.dump(project_structure, f, ensure_ascii=False, indent=4)\\n    \\n    data = {}\\n    # Leer y analizar el archivo JSON\\n    try:\\n       with open(output_path, \\'r\\') as file:\\n        data = json.load(file)\\n    except Exception as e:\\n        print(f\"Error al leer el archivo JSON: {e}\")\\n\\n    # Proceso de extracción de fragmentos de código\\n    code_snippets = []\\n    try:\\n    # Iterar sobre todas las claves principales del diccionario JSON\\n        for main_key in data.keys():\\n            for filename, content in data[main_key].items():\\n                if \\'functions\\' in content:\\n                    for function_name, function_content in content[\\'functions\\'].items():\\n                        if \\'code\\' in function_content:\\n                            code_snippets.append(function_content[\\'code\\'])\\n    except Exception as e:\\n        code_snippets = f\"Error al procesar el JSON: {e}\"\\n\\n    searcher = CodeSearcher(args.model_path, code_snippets)\\n    k=3\\n    t = searcher.get_similarity_search(args.nl_query, k)\\n\\n    print(\"Top K similar items: \\\\n\")\\n    print(t)',\n",
       "     'lines': {'start': 66, 'end': 118},\n",
       "     'docstring': None}},\n",
       "   'classes': {},\n",
       "   'direct_imports': {'argparse': {'alias': None, 'lineno': 1},\n",
       "    'os': {'alias': None, 'lineno': 2},\n",
       "    'json': {'alias': None, 'lineno': 3},\n",
       "    'subprocess': {'alias': None, 'lineno': 6},\n",
       "    'shutil': {'alias': None, 'lineno': 8},\n",
       "    'tempfile': {'alias': None, 'lineno': 9}},\n",
       "   'from_imports': {'extract_code': [{'name': '*',\n",
       "      'alias': None,\n",
       "      'lineno': 4}],\n",
       "    'code_search': [{'name': 'CodeSearcher', 'alias': None, 'lineno': 5}]},\n",
       "   'module_docstring': None},\n",
       "  'main.py': {'functions': {'main': {'code': 'def main():\\n    parser = argparse.ArgumentParser(description=\\'Parse a project structure into a JSON representation.\\')\\n    parser.add_argument(\\'--project_path\\', type=str, required=True, help=\\'Path to the root of the project.\\')\\n    parser.add_argument(\\'--output_dir_path\\', type=str, required=True, help=\\'Path to the output dir.\\')\\n    parser.add_argument(\\'--model_path\\', type=str, default= r\"C:\\\\Users\\\\agianolini\\\\OneDrive - ANDES WEALTH MANAGEMENT SA\\\\Desktop\\\\research-assistant-main\\\\model (7).bin\") # change to your model path\\n    parser.add_argument(\\'--nl_query\\', type=str, default=\\'a for loop that prints the world\\')\\n    args = parser.parse_args()\\n    \\n    # Get the project structure\\n    project_structure = get_project_structure(args.project_path)\\n\\n    # Save the project structure to a JSON file\\n    os.makedirs(args.output_dir_path, exist_ok=True)\\n    project_name = os.path.basename(args.project_path)\\n    output_path = os.path.join(args.output_dir_path, f\\'{project_name}.json\\')\\n    with open(output_path, \\'w\\', encoding=\\'utf-8\\') as f:\\n        json.dump(project_structure, f, ensure_ascii=False, indent=4)\\n    \\n    data = {}\\n    # Leer y analizar el archivo JSON\\n    try:\\n       with open(output_path, \\'r\\') as file:\\n        data = json.load(file)\\n    except Exception as e:\\n        print(f\"Error al leer el archivo JSON: {e}\")\\n\\n    # Proceso de extracción de fragmentos de código\\n    code_snippets = []\\n    try:\\n    # Iterar sobre todas las claves principales del diccionario JSON\\n        for main_key in data.keys():\\n            for filename, content in data[main_key].items():\\n                if \\'functions\\' in content:\\n                    for function_name, function_content in content[\\'functions\\'].items():\\n                        if \\'code\\' in function_content:\\n                            code_snippets.append(function_content[\\'code\\'])\\n    except Exception as e:\\n        code_snippets = f\"Error al procesar el JSON: {e}\"\\n\\n    searcher = CodeSearcher(args.model_path, code_snippets)\\n    k=3\\n    t = searcher.get_similarity_search(args.nl_query, k)\\n\\n    print(\"Top K similar items: \\\\n\")\\n    print(t)',\n",
       "     'lines': {'start': 7, 'end': 52},\n",
       "     'docstring': None}},\n",
       "   'classes': {},\n",
       "   'direct_imports': {'argparse': {'alias': None, 'lineno': 1},\n",
       "    'os': {'alias': None, 'lineno': 2},\n",
       "    'json': {'alias': None, 'lineno': 3}},\n",
       "   'from_imports': {'extract_code': [{'name': 'get_project_structure',\n",
       "      'alias': None,\n",
       "      'lineno': 4},\n",
       "     {'name': 'analyze_dependencies', 'alias': None, 'lineno': 4},\n",
       "     {'name': 'process_atlas_data', 'alias': None, 'lineno': 4}],\n",
       "    'code_search': [{'name': 'CodeSearcher', 'alias': None, 'lineno': 5}]},\n",
       "   'module_docstring': None},\n",
       "  'pdfAnalyzer.py': {'functions': {'get_toc': {'code': 'def get_toc(doc):\\n    toc = doc.get_toc()\\n    json_toc = []\\n    for t in toc:\\n        json_toc.append({\"lvl\": t[0], \"title\": t[1], \"page\": t[2]})\\n    page_ranges = { json_toc[i][\\'title\\']: (json_toc[i][\\'page\\'], json_toc[i+1][\\'page\\']) for i in  range(len(json_toc)-1) }\\n    return page_ranges',\n",
       "     'lines': {'start': 6, 'end': 13},\n",
       "     'docstring': None},\n",
       "    'extract_text': {'code': 'def extract_text(doc, page):\\n    text = \"\"\\n    for page in doc:\\n        text += page.getText()\\n    return text',\n",
       "     'lines': {'start': 14, 'end': 19},\n",
       "     'docstring': None},\n",
       "    'extract_pages': {'code': 'def extract_pages(doc, start, end):\\n    res = []\\n    start = max (int(start), 0)\\n    end = min (int(end), len(doc))\\n\\n    for i in range(start, end+1):\\n        res.append(doc[i].get_text(\"text\").strip())\\n    return res',\n",
       "     'lines': {'start': 20, 'end': 28},\n",
       "     'docstring': None},\n",
       "    'get_args': {'code': 'def get_args():\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\"pdf_file\", help=\"The pdf file to be processed\")\\n    parser.add_argument(\"operation\", help=\"The operation to be performed\")\\n    parser.add_argument(\"start\",help=\"The starting page\")\\n    parser.add_argument(\"end\",help=\"The ending page\")\\n\\n    args = parser.parse_args()\\n    return args',\n",
       "     'lines': {'start': 30, 'end': 39},\n",
       "     'docstring': None}},\n",
       "   'classes': {},\n",
       "   'direct_imports': {'fitz': {'alias': None, 'lineno': 1},\n",
       "    'sys': {'alias': None, 'lineno': 2},\n",
       "    'json': {'alias': None, 'lineno': 3},\n",
       "    'argparse': {'alias': None, 'lineno': 4}},\n",
       "   'from_imports': {},\n",
       "   'module_docstring': None},\n",
       "  'unixcoder_ft_codesearch_CSN_python_inference_and_testing.ipynb': {'cells': [['import torch.nn as nn\\n',\n",
       "     'import torch\\n',\n",
       "     'from transformers import RobertaTokenizer, RobertaModel\\n',\n",
       "     'import numpy as np\\n',\n",
       "     'from scipy.spatial.distance import cdist'],\n",
       "    ['class Model(nn.Module):\\n',\n",
       "     '    def __init__(self, encoder):\\n',\n",
       "     '        super(Model, self).__init__()\\n',\n",
       "     '        self.encoder = encoder\\n',\n",
       "     '\\n',\n",
       "     '    def forward(self, code_inputs=None, nl_inputs=None):\\n',\n",
       "     '        if code_inputs is not None:\\n',\n",
       "     '            outputs = self.encoder(code_inputs,attention_mask=code_inputs.ne(1))[0]\\n',\n",
       "     '            outputs = (outputs*code_inputs.ne(1)[:,:,None]).sum(1)/code_inputs.ne(1).sum(-1)[:,None]\\n',\n",
       "     '            return torch.nn.functional.normalize(outputs, p=2, dim=1)\\n',\n",
       "     '        else:\\n',\n",
       "     '            outputs = self.encoder(nl_inputs,attention_mask=nl_inputs.ne(1))[0]\\n',\n",
       "     '            outputs = (outputs*nl_inputs.ne(1)[:,:,None]).sum(1)/nl_inputs.ne(1).sum(-1)[:,None]\\n',\n",
       "     '            return torch.nn.functional.normalize(outputs, p=2, dim=1)\\n'],\n",
       "    ['from google.colab import drive\\n', \"drive.mount('/content/drive')\\n\"],\n",
       "    ['# Path to your model.bin file on Google Drive\\n',\n",
       "     \"# model_path = '/content/drive/My Drive/model (7).bin'\\n\",\n",
       "     'model_path = r\"C:\\\\Users\\\\agianolini\\\\OneDrive - ANDES WEALTH MANAGEMENT SA\\\\Desktop\\\\research-assistant-main\\\\model (7).bin\"'],\n",
       "    ['class CodeSearcher:\\n',\n",
       "     '    def __init__(self, model_path, code_snippets, device=None):\\n',\n",
       "     '        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\\n',\n",
       "     \"        self.tokenizer = RobertaTokenizer.from_pretrained('microsoft/unixcoder-base')\\n\",\n",
       "     '        self.model = self.load_model(model_path)\\n',\n",
       "     '        self.code_snippets = code_snippets\\n',\n",
       "     '        self.code_embeddings = self.generate_code_embeddings(code_snippets)\\n',\n",
       "     '\\n',\n",
       "     '    def load_model(self, model_path):\\n',\n",
       "     \"        model = RobertaModel.from_pretrained('microsoft/unixcoder-base')\\n\",\n",
       "     '        model = Model(model)\\n',\n",
       "     '        model.load_state_dict(torch.load(model_path, map_location=self.device))\\n',\n",
       "     '        model.to(self.device)\\n',\n",
       "     '        model.eval()\\n',\n",
       "     '        return model\\n',\n",
       "     '\\n',\n",
       "     '    def generate_code_embeddings(self, code_snippets):\\n',\n",
       "     '        embeddings = []\\n',\n",
       "     '        for snippet in code_snippets:\\n',\n",
       "     \"            inputs = self.tokenizer.encode_plus(snippet, add_special_tokens=True, max_length=256, truncation=True, padding='max_length', return_tensors='pt')\\n\",\n",
       "     '            with torch.no_grad():\\n',\n",
       "     \"                embedding = self.model(code_inputs=inputs['input_ids'].to(self.device))\\n\",\n",
       "     '            embeddings.append(embedding.cpu().numpy())\\n',\n",
       "     '        return np.vstack(embeddings)\\n',\n",
       "     '\\n',\n",
       "     '    def get_query_embedding(self, query):\\n',\n",
       "     \"        inputs = self.tokenizer.encode_plus(query, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\\n\",\n",
       "     '        with torch.no_grad():\\n',\n",
       "     \"            embedding = self.model(code_inputs=inputs['input_ids'].to(self.device))\\n\",\n",
       "     '        return embedding.cpu().numpy()\\n',\n",
       "     '\\n',\n",
       "     '    def get_similarity_search(self, query, k):\\n',\n",
       "     '        query_embedding = self.get_query_embedding(query)\\n',\n",
       "     \"        similarities = 1 - cdist(query_embedding, self.code_embeddings, 'cosine').flatten()\\n\",\n",
       "     '\\n',\n",
       "     '        # Get top-k indices\\n',\n",
       "     '        top_k_indices = np.argsort(similarities)[-k:]\\n',\n",
       "     '\\n',\n",
       "     '        # Create a dictionary for the top k results\\n',\n",
       "     '        results = {}\\n',\n",
       "     '        for index in reversed(top_k_indices):\\n',\n",
       "     '            snippet_info = {\\n',\n",
       "     \"                'index': index,\\n\",\n",
       "     \"                'snippet': self.code_snippets[index],\\n\",\n",
       "     \"                'similarity': similarities[index]\\n\",\n",
       "     '            }\\n',\n",
       "     \"            results[f'top_{k}'] = snippet_info\\n\",\n",
       "     '            k -= 1\\n',\n",
       "     '\\n',\n",
       "     '        return results\\n',\n",
       "     '\\n',\n",
       "     '\\n'],\n",
       "    ['# Example usage\\n',\n",
       "     'code_snippets = [\"print(\\'Hello, world!\\')\", \"my ass is like the sky!\", \"for i in range(10): print(i)\"]  # Your collection of code snippets\\n',\n",
       "     'searcher = CodeSearcher(model_path, code_snippets)'],\n",
       "    ['query = \"print hello world\"\\n',\n",
       "     'k = 3\\n',\n",
       "     't = searcher.get_similarity_search(query, k)\\n',\n",
       "     '\\n',\n",
       "     'print(\"Top K similar items: \\\\n\")\\n',\n",
       "     't']]}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_project_structure(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "def get_project_structure(project_path):\n",
    "    res = {}\n",
    "    repo_name = os.path.basename(project_path)\n",
    "\n",
    "    for root, dirs, files in os.walk(project_path):\n",
    "        # Trim the root to start from the repository's root directory\n",
    "        trimmed_root = root[len(project_path):].lstrip(os.sep)\n",
    "        path_parts = trimmed_root.split(os.sep)\n",
    "        current_level = res\n",
    "\n",
    "        # Construct the nested dictionary structure\n",
    "        for part in path_parts:\n",
    "            current_level = current_level.setdefault(part, {})\n",
    "\n",
    "        for file in files:\n",
    "            try:\n",
    "                file_path = os.path.join(root, file)\n",
    "                if file.endswith(\".py\") or file.endswith(\".ipynb\"):\n",
    "                    current_level[file] = get_file_code(file_path)\n",
    "                elif file.endswith(\".md\"):\n",
    "                    current_level[file] = parse_md(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process file {file_path}: {e}\")\n",
    "\n",
    "    # Wrap in the repository's name\n",
    "    return {repo_name: res}\n",
    "\n",
    "def clone_github_repo(github_url, dest_folder):\n",
    "    \"\"\"\n",
    "    Clone a GitHub repository to the specified local directory, even if the directory is not empty.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a temporary directory to clone the repo\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Clone the repository into the temporary directory\n",
    "            subprocess.run([\"git\", \"clone\", github_url, temp_dir], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            \n",
    "            # Move the contents of the cloned repo to the destination folder\n",
    "            repo_name = os.path.basename(github_url.rstrip('/').split('/')[-1])\n",
    "            source_folder = os.path.join(temp_dir, repo_name)\n",
    "            for item in os.listdir(source_folder):\n",
    "                s = os.path.join(source_folder, item)\n",
    "                d = os.path.join(dest_folder, item)\n",
    "                if os.path.isdir(s):\n",
    "                    shutil.copytree(s, d, dirs_exist_ok=True)\n",
    "                else:\n",
    "                    shutil.copy2(s, d)\n",
    "            \n",
    "            print(\"Repository cloned successfully.\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error in subprocess: {e.stderr.decode().strip()}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
