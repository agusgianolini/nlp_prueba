{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_code import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba_json = get_project_structure(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    def __init__(self, model_path, code_snippets, device=None):\\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\\n        self.tokenizer = RobertaTokenizer.from_pretrained(\\'microsoft/unixcoder-',\n",
       " \"base')\\n        self.model = self.load_model(model_path)\\n        self.code_snippets = code_snippets\\n        self.code_embeddings = self.generate_code_embeddings(code_snippets)\",\n",
       " '    def forward(self, code_inputs=None, nl_inputs=None):\\n        if code_inputs is not None:\\n            outputs = self.encoder(code_inputs,attention_mask=code_inputs.ne(1))[0]\\n            outputs = (outputs*code_inputs.ne(1)[:,:,None]).sum(1)/code_inputs.',\n",
       " 'ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)\\n        else:\\n            outputs = self.encoder(nl_inputs,attention_mask=nl_inputs.ne(1))[0]\\n            outputs = (outputs*nl_inputs.ne(1)[:,:,None]).sum(1)/nl_in',\n",
       " 'puts.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)',\n",
       " \"    def load_model(self, model_path):\\n        model = RobertaModel.from_pretrained('microsoft/unixcoder-base')\\n        model = Model(model)\\n        model.load_state_dict(torch.load(model_path, map_location=self.device))\\n        model.to(self.device)\\n      \",\n",
       " '  model.eval()\\n        return model',\n",
       " \"    def generate_code_embeddings(self, code_snippets):\\n        embeddings = []\\n        for snippet in code_snippets:\\n            inputs = self.tokenizer.encode_plus(snippet, add_special_tokens=True, max_length=256, truncation=True, padding='max_length', re\",\n",
       " \"turn_tensors='pt')\\n            with torch.no_grad():\\n                embedding = self.model(code_inputs=inputs['input_ids'].to(self.device))\\n            embeddings.append(embedding.cpu().numpy())\\n        return np.vstack(embeddings)\",\n",
       " \"    def get_query_embedding(self, query):\\n        inputs = self.tokenizer.encode_plus(query, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\\n        with torch.no_grad():\\n            embedding = self.mod\",\n",
       " \"el(code_inputs=inputs['input_ids'].to(self.device))\\n        return embedding.cpu().numpy()\",\n",
       " \"    def get_similarity_search(self, query, k):\\n        query_embedding = self.get_query_embedding(query)\\n        similarities = 1 - cdist(query_embedding, self.code_embeddings, 'cosine').flatten()\\n\\n        # Get top-k indices\\n        top_k_indices = np.arg\",\n",
       " \"sort(similarities)[-k:]\\n\\n        # Create a dictionary for the top k results\\n        results = {}\\n        for index in reversed(top_k_indices):\\n            snippet_info = {\\n                'index': index,\\n                'snippet': self.code_snippets[index\",\n",
       " \"],\\n                'similarity': similarities[index]\\n            }\\n            results[f'top_{k}'] = snippet_info\\n            k -= 1\\n    \\n        return results\",\n",
       " 'class Model(nn.Module):\\n    def __init__(self, encoder):\\n        super(Model, self).__init__()\\n        self.encoder = encoder\\n\\n    def forward(self, code_inputs=None, nl_inputs=None):\\n        if code_inputs is not None:\\n            outputs = self.encoder(c',\n",
       " 'ode_inputs,attention_mask=code_inputs.ne(1))[0]\\n            outputs = (outputs*code_inputs.ne(1)[:,:,None]).sum(1)/code_inputs.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)\\n        else:\\n            outputs = s',\n",
       " 'elf.encoder(nl_inputs,attention_mask=nl_inputs.ne(1))[0]\\n            outputs = (outputs*nl_inputs.ne(1)[:,:,None]).sum(1)/nl_inputs.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)',\n",
       " 'class CodeSearcher:\\n    def __init__(self, model_path, code_snippets, device=None):\\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\\n        self.tokenizer = RobertaTokenizer.from_pretrained(\\'',\n",
       " \"microsoft/unixcoder-base')\\n        self.model = self.load_model(model_path)\\n        self.code_snippets = code_snippets\\n        self.code_embeddings = self.generate_code_embeddings(code_snippets)\\n\\n    def load_model(self, model_path):\\n        model = Robert\",\n",
       " \"aModel.from_pretrained('microsoft/unixcoder-base')\\n        model = Model(model)\\n        model.load_state_dict(torch.load(model_path, map_location=self.device))\\n        model.to(self.device)\\n        model.eval()\\n        return model\\n\\n    def generate_code_e\",\n",
       " \"mbeddings(self, code_snippets):\\n        embeddings = []\\n        for snippet in code_snippets:\\n            inputs = self.tokenizer.encode_plus(snippet, add_special_tokens=True, max_length=256, truncation=True, padding='max_length', return_tensors='pt')\\n    \",\n",
       " \"        with torch.no_grad():\\n                embedding = self.model(code_inputs=inputs['input_ids'].to(self.device))\\n            embeddings.append(embedding.cpu().numpy())\\n        return np.vstack(embeddings)\\n\\n    def get_query_embedding(self, query):\\n   \",\n",
       " \"     inputs = self.tokenizer.encode_plus(query, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\\n        with torch.no_grad():\\n            embedding = self.model(code_inputs=inputs['input_ids'].to(self.de\",\n",
       " \"vice))\\n        return embedding.cpu().numpy()\\n\\n    def get_similarity_search(self, query, k):\\n        query_embedding = self.get_query_embedding(query)\\n        similarities = 1 - cdist(query_embedding, self.code_embeddings, 'cosine').flatten()\\n\\n        # G\",\n",
       " \"et top-k indices\\n        top_k_indices = np.argsort(similarities)[-k:]\\n\\n        # Create a dictionary for the top k results\\n        results = {}\\n        for index in reversed(top_k_indices):\\n            snippet_info = {\\n                'index': index,\\n    \",\n",
       " \"            'snippet': self.code_snippets[index],\\n                'similarity': similarities[index]\\n            }\\n            results[f'top_{k}'] = snippet_info\\n            k -= 1\\n    \\n        return results\",\n",
       " '    def __init__(self, model_path, code_snippets, device=None):\\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\\n        self.tokenizer = RobertaTokenizer.from_pretrained(\\'microsoft/unixcoder-',\n",
       " \"base')\\n        self.model = self.load_model(model_path)\\n        self.code_snippets = code_snippets\\n        self.code_embeddings = self.generate_code_embeddings(code_snippets)\",\n",
       " '    def forward(self, code_inputs=None, nl_inputs=None):\\n        if code_inputs is not None:\\n            outputs = self.encoder(code_inputs,attention_mask=code_inputs.ne(1))[0]\\n            outputs = (outputs*code_inputs.ne(1)[:,:,None]).sum(1)/code_inputs.',\n",
       " 'ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)\\n        else:\\n            outputs = self.encoder(nl_inputs,attention_mask=nl_inputs.ne(1))[0]\\n            outputs = (outputs*nl_inputs.ne(1)[:,:,None]).sum(1)/nl_in',\n",
       " 'puts.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)',\n",
       " \"    def load_model(self, model_path):\\n        model = RobertaModel.from_pretrained('microsoft/unixcoder-base')\\n        model = Model(model)\\n        model.load_state_dict(torch.load(model_path, map_location=self.device))\\n        model.to(self.device)\\n      \",\n",
       " '  model.eval()\\n        return model',\n",
       " \"    def generate_code_embeddings(self, code_snippets):\\n        embeddings = []\\n        for snippet in code_snippets:\\n            inputs = self.tokenizer.encode_plus(snippet, add_special_tokens=True, max_length=256, truncation=True, padding='max_length', re\",\n",
       " \"turn_tensors='pt')\\n            with torch.no_grad():\\n                embedding = self.model(code_inputs=inputs['input_ids'].to(self.device))\\n            embeddings.append(embedding.cpu().numpy())\\n        return np.vstack(embeddings)\",\n",
       " \"    def get_query_embedding(self, query):\\n        inputs = self.tokenizer.encode_plus(query, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\\n        with torch.no_grad():\\n            embedding = self.mod\",\n",
       " \"el(code_inputs=inputs['input_ids'].to(self.device))\\n        return embedding.cpu().numpy()\",\n",
       " \"    def get_similarity_search(self, query, k):\\n        query_embedding = self.get_query_embedding(query)\\n        similarities = 1 - cdist(query_embedding, self.code_embeddings, 'cosine').flatten()\\n\\n        # Get top-k indices\\n        top_k_indices = np.arg\",\n",
       " \"sort(similarities)[-k:]\\n\\n        # Create a dictionary for the top k results\\n        results = {}\\n        for index in reversed(top_k_indices):\\n            snippet_info = {\\n                'index': index,\\n                'snippet': self.code_snippets[index\",\n",
       " \"],\\n                'similarity': similarities[index]\\n            }\\n            results[f'top_{k}'] = snippet_info\\n            k -= 1\\n\\n        return results\",\n",
       " 'class Model(nn.Module):\\n    def __init__(self, encoder):\\n        super(Model, self).__init__()\\n        self.encoder = encoder\\n\\n    def forward(self, code_inputs=None, nl_inputs=None):\\n        if code_inputs is not None:\\n            outputs = self.encoder(c',\n",
       " 'ode_inputs,attention_mask=code_inputs.ne(1))[0]\\n            outputs = (outputs*code_inputs.ne(1)[:,:,None]).sum(1)/code_inputs.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)\\n        else:\\n            outputs = s',\n",
       " 'elf.encoder(nl_inputs,attention_mask=nl_inputs.ne(1))[0]\\n            outputs = (outputs*nl_inputs.ne(1)[:,:,None]).sum(1)/nl_inputs.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)',\n",
       " 'class CodeSearcher:\\n    def __init__(self, model_path, code_snippets, device=None):\\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\\n        self.tokenizer = RobertaTokenizer.from_pretrained(\\'',\n",
       " \"microsoft/unixcoder-base')\\n        self.model = self.load_model(model_path)\\n        self.code_snippets = code_snippets\\n        self.code_embeddings = self.generate_code_embeddings(code_snippets)\\n\\n    def load_model(self, model_path):\\n        model = Robert\",\n",
       " \"aModel.from_pretrained('microsoft/unixcoder-base')\\n        model = Model(model)\\n        model.load_state_dict(torch.load(model_path, map_location=self.device))\\n        model.to(self.device)\\n        model.eval()\\n        return model\\n\\n    def generate_code_e\",\n",
       " \"mbeddings(self, code_snippets):\\n        embeddings = []\\n        for snippet in code_snippets:\\n            inputs = self.tokenizer.encode_plus(snippet, add_special_tokens=True, max_length=256, truncation=True, padding='max_length', return_tensors='pt')\\n    \",\n",
       " \"        with torch.no_grad():\\n                embedding = self.model(code_inputs=inputs['input_ids'].to(self.device))\\n            embeddings.append(embedding.cpu().numpy())\\n        return np.vstack(embeddings)\\n\\n    def get_query_embedding(self, query):\\n   \",\n",
       " \"     inputs = self.tokenizer.encode_plus(query, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\\n        with torch.no_grad():\\n            embedding = self.model(code_inputs=inputs['input_ids'].to(self.de\",\n",
       " \"vice))\\n        return embedding.cpu().numpy()\\n\\n    def get_similarity_search(self, query, k):\\n        query_embedding = self.get_query_embedding(query)\\n        similarities = 1 - cdist(query_embedding, self.code_embeddings, 'cosine').flatten()\\n\\n        # G\",\n",
       " \"et top-k indices\\n        top_k_indices = np.argsort(similarities)[-k:]\\n\\n        # Create a dictionary for the top k results\\n        results = {}\\n        for index in reversed(top_k_indices):\\n            snippet_info = {\\n                'index': index,\\n    \",\n",
       " \"            'snippet': self.code_snippets[index],\\n                'similarity': similarities[index]\\n            }\\n            results[f'top_{k}'] = snippet_info\\n            k -= 1\\n\\n        return results\",\n",
       " 'def code_to_json(code_lines, tree):\\n    res = {}\\n    functions = {}\\n    classes = {}\\n    direct_imports = {}\\n    from_imports = defaultdict(list)\\n\\n    # Extract module-level docstring, if present\\n    module_docstring = ast.get_docstring(tree)\\n\\n    for node',\n",
       " ' in ast.walk(tree):\\n        if isinstance(node, ast.FunctionDef):\\n            function_code = \"\\\\n\".join(code_lines[node.lineno - 1: node.end_lineno])\\n            function_lines = {\"start\": node.lineno - 1, \"end\": node.end_lineno}\\n            function_docst',\n",
       " 'ring = ast.get_docstring(node)\\n            functions[node.name] = {\"code\": function_code, \"lines\": function_lines, \"docstring\": function_docstring}\\n        elif isinstance(node, ast.ClassDef):\\n            class_code = \"\\\\n\".join(code_lines[node.lineno - 1: ',\n",
       " 'node.end_lineno])\\n            class_lines = {\"start\": node.lineno - 1, \"end\": node.end_lineno}\\n            class_docstring = ast.get_docstring(node)\\n            classes[node.name] = {\"code\": class_code, \"lines\": class_lines, \"docstring\": class_docstring}\\n ',\n",
       " '       elif isinstance(node, ast.Import):\\n            for alias in node.names:\\n                direct_imports[alias.name] = {\"alias\": alias.asname, \"lineno\": node.lineno}\\n        elif isinstance(node, ast.ImportFrom):\\n            for alias in node.names:\\n ',\n",
       " '               from_imports[node.module].append({\\n                    \"name\": alias.name,\\n                    \"alias\": alias.asname,\\n                    \"lineno\": node.lineno\\n                })\\n\\n    res[\"functions\"] = functions\\n    res[\"classes\"] = classes',\n",
       " '\\n    res[\"direct_imports\"] = direct_imports\\n    res[\"from_imports\"] = dict(from_imports)  # Convert back to a regular dict for JSON serialization\\n    res[\"module_docstring\"] = module_docstring\\n\\n    return res',\n",
       " \"def get_file_code(file_path):\\n    with open(file_path, 'r', encoding='utf-8') as f:\\n        if file_path.endswith('.py'):\\n            code = f.read()\\n            code_lines = code.split('\\\\n')\\n            tree = ast.parse(code)\\n            res = code_to_jso\",\n",
       " \"n(code_lines, tree)\\n        elif file_path.endswith('.ipynb'):\\n            notebook = json.load(f)\\n            res = {'cells': [cell['source'] for cell in notebook['cells'] if cell['cell_type'] == 'code']}\\n    return res\",\n",
       " \"def parse_md(file_path):\\n    with open(file_path, 'r', encoding='utf-8') as f:\\n        return markdown.markdown(f.read())\",\n",
       " \"def get_project_structure(project_path):\\n    res = {}\\n    file_path = None\\n\\n    project_name = os.path.basename(project_path.rstrip(os.sep)) # Get the project folder name\\n\\n    for root, dirs, files in os.walk(project_path):\\n        # Skip if it's before th\",\n",
       " 'e project directory\\n        if project_name not in root:\\n            continue\\n\\n        path_parts = root.split(os.sep)\\n        # Start from the project directory in the path_parts\\n        start_index = path_parts.index(project_name)\\n        relevant_path_p',\n",
       " 'arts = path_parts[start_index:]\\n\\n        current_level = res\\n        for part in relevant_path_parts:\\n            if part not in current_level:\\n                current_level[part] = {}\\n            current_level = current_level[part]\\n        structure = cur',\n",
       " 'rent_level\\n\\n        for directory in dirs:\\n            structure[directory] = {}\\n\\n        for file in files:\\n            try:\\n                file_path = os.path.join(root, file)\\n                if file.endswith(\".py\") or file.endswith(\".ipynb\"):\\n         ',\n",
       " '           structure[file] = get_file_code(file_path)\\n                elif file.endswith(\".md\"):\\n                    structure[file] = parse_md(file_path)\\n            except Exception as e:\\n                print(f\"Failed to process file {file_path}: {e}\")\\n',\n",
       " '\\n    return res',\n",
       " \"def summarize_structure(project_structure, indent=''):\\n    python_files = 0\\n    ipynb_files = 0\\n    md_files = 0\\n    function_counts = []\\n    class_counts = []\\n    import_counts = []\\n\\n    for key, value in project_structure.items():\\n        if isinstance(v\",\n",
       " \"alue, dict):\\n            if key.endswith('.py') or key.endswith('.ipynb'):\\n                python_files += 1\\n                function_counts.append(len(value.get('functions', {})))\\n                class_counts.append(len(value.get('classes', {})))\\n        \",\n",
       " \"        import_counts.append(len(value.get('imports', {})))\\n            elif key.endswith('.md'):\\n                md_files += 1\\n            else:\\n                # recurse into subdirectories\\n                sub_python_files, sub_ipynb_files, sub_md_files,\",\n",
       " \" sub_function_counts, sub_class_counts, sub_import_counts = summarize_structure(\\n                    value, indent + '  ')\\n                python_files += sub_python_files\\n                ipynb_files += sub_ipynb_files\\n                md_files += sub_md_fi\",\n",
       " 'les\\n                function_counts.extend(sub_function_counts)\\n                class_counts.extend(sub_class_counts)\\n                import_counts.extend(sub_import_counts)\\n\\n    return python_files, ipynb_files, md_files, function_counts, class_counts, im',\n",
       " 'port_counts',\n",
       " 'def get_imports(tree):\\n    imports = set()\\n\\n    for node in ast.walk(tree):\\n        if isinstance(node, ast.Import):\\n            for alias in node.names:\\n                imports.add(alias.name)\\n        elif isinstance(node, ast.ImportFrom):\\n            if ',\n",
       " \"node.level > 0:\\n                # We have a relative import, which we'll ignore for now\\n                continue\\n\\n            module = node.module\\n            for alias in node.names:\\n                if module is None:\\n                    imports.add(alias\",\n",
       " '.name)\\n                else:\\n                    imports.add(f\"{module}.{alias.name}\")\\n\\n    return imports',\n",
       " \"def analyze_codebase(directory):\\n    results = {}\\n\\n    # Step 2-3: Traverse directory and parse each Python file\\n    for file_path in traverse_directory(directory):\\n        with open(file_path, 'r') as f:\\n            code = f.read()\\n        tree = ast.pars\",\n",
       " 'e(code)\\n        code_lines = code.splitlines()\\n        results[file_path] = code_to_json(code_lines, tree)\\n\\n    # Step 4-5: Analyze imports to find dependencies\\n    dependencies = analyze_dependencies(results)\\n\\n    return results, dependencies',\n",
       " 'def traverse_directory(directory):\\n    for root, dirs, files in os.walk(directory):\\n        for file in files:\\n            if file.endswith(\".py\"):\\n                yield os.path.join(root, file)',\n",
       " \"def is_external(module):\\n    try:\\n        spec = importlib.util.find_spec(module)\\n        return spec is not None and 'site-packages' in spec.origin\\n    except ModuleNotFoundError:\\n        return False\",\n",
       " \"def analyze_dependencies(results):\\n    dependencies = {file: set() for file in results.keys()}\\n\\n    for file, structure in results.items():\\n        if 'direct_imports' in structure:\\n            for imported_module in structure['direct_imports']:\\n          \",\n",
       " '      if not is_external(imported_module):\\n                    # Note: This code assumes the imported file is in the same directory\\n                    # This might not be the case in your project.\\n                    dependencies[file].add(imported_module',\n",
       " ')\\n\\n    return dependencies',\n",
       " 'def process_atlas_data(atlas_data):\\n    # Placeholder for the documents\\n    documents = []\\n\\n    # Iterate over the files in the atlas data\\n    for file_path, file_data in atlas_data.items():\\n        # Check if the file data is a dictionary\\n        if isins',\n",
       " 'tance(file_data, dict):\\n            # Process the functions\\n            for function_name, function_data in file_data.get(\\'functions\\', {}).items():\\n                # Construct the document ID\\n                document_id = f\"{file_path}/function/{function_n',\n",
       " 'ame}\"\\n\\n                # Prepare the document content and metadata\\n                document_content = json.dumps(function_data)\\n                document_metadata = {\\'file_path\\': file_path, \\'type\\': \\'function\\', \\'name\\': function_name}\\n\\n                # Add t',\n",
       " \"he document to the list\\n                documents.append((document_id, document_content, document_metadata))\\n\\n            # Process the classes\\n            for class_name, class_data in file_data.get('classes', {}).items():\\n                # Construct the \",\n",
       " 'document ID\\n                document_id = f\"{file_path}/class/{class_name}\"\\n\\n                # Prepare the document content and metadata\\n                document_content = json.dumps(class_data)\\n                document_metadata = {\\'file_path\\': file_path, ',\n",
       " \"'type': 'class', 'name': class_name}\\n\\n                # Add the document to the list\\n                documents.append((document_id, document_content, document_metadata))\\n\\n    return documents\",\n",
       " \"def get_project_structure(project_path):\\n    res = {}\\n    repo_name = os.path.basename(project_path)\\n\\n    for root, dirs, files in os.walk(project_path):\\n        # Trim the root to start from the repository's root directory\\n        trimmed_root = root[len(\",\n",
       " 'project_path):].lstrip(os.sep)\\n        path_parts = trimmed_root.split(os.sep)\\n        current_level = res\\n\\n        # Construct the nested dictionary structure\\n        for part in path_parts:\\n            current_level = current_level.setdefault(part, {})\\n\\n',\n",
       " '        for file in files:\\n            try:\\n                file_path = os.path.join(root, file)\\n                if file.endswith(\".py\") or file.endswith(\".ipynb\"):\\n                    current_level[file] = get_file_code(file_path)\\n                elif fil',\n",
       " 'e.endswith(\".md\"):\\n                    current_level[file] = parse_md(file_path)\\n            except Exception as e:\\n                print(f\"Failed to process file {file_path}: {e}\")\\n\\n    # Wrap in the repository\\'s name\\n    return {repo_name: res}',\n",
       " 'def clone_github_repo(github_url, dest_folder):\\n    \"\"\"\\n    Clone a GitHub repository to the specified local directory, even if the directory is not empty.\\n    \"\"\"\\n    try:\\n        # Create a temporary directory to clone the repo\\n        with tempfile.Temp',\n",
       " 'oraryDirectory() as temp_dir:\\n            # Clone the repository into the temporary directory\\n            subprocess.run([\"git\", \"clone\", github_url, temp_dir], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\\n            \\n            # Move the',\n",
       " \" contents of the cloned repo to the destination folder\\n            repo_name = os.path.basename(github_url.rstrip('/').split('/')[-1])\\n            source_folder = os.path.join(temp_dir, repo_name)\\n            for item in os.listdir(source_folder):\\n        \",\n",
       " '        s = os.path.join(source_folder, item)\\n                d = os.path.join(dest_folder, item)\\n                if os.path.isdir(s):\\n                    shutil.copytree(s, d, dirs_exist_ok=True)\\n                else:\\n                    shutil.copy2(s, d',\n",
       " ')\\n            \\n            print(\"Repository cloned successfully.\")\\n\\n    except subprocess.CalledProcessError as e:\\n        print(f\"Error in subprocess: {e.stderr.decode().strip()}\")\\n        raise\\n    except Exception as e:\\n        print(f\"Error: {e}\")\\n   ',\n",
       " '     raise',\n",
       " \"def main():\\n    parser = argparse.ArgumentParser(description='Parse a project structure into a JSON representation.')\\n    # parser.add_argument('--github_url', type=str, required=True, help='URL to the GitHub repository.')    \\n    parser.add_argument('--gi\",\n",
       " 'thub_url\\', type=str, default= \"https://github.com/dr-aheydari/SoftAdapt\", help=\\'URL to the GitHub repository.\\')    \\n    # parser.add_argument(\\'--pdf_path\\', type=str, required=True, help=\\'Path to the paper PDF\\')\\n    # parser.add_argument(\\'--output_dir_path\\'',\n",
       " ', type=str, required=True, help=\\'Path to the output dir.\\')\\n    parser.add_argument(\\'--output_dir_path\\', type=str, default= r\"C:\\\\Users\\\\agianolini\\\\OneDrive - ANDES WEALTH MANAGEMENT SA\\\\Desktop\\\\research-assistant-main\", \\n                        help=\\'Path to ',\n",
       " 'the output dir.\\')\\n    parser.add_argument(\\'--model_path\\', type=str, default=r\"C:\\\\Users\\\\agianolini\\\\OneDrive - ANDES WEALTH MANAGEMENT SA\\\\Desktop\\\\research-assistant-main\\\\model (7).bin\") # change to your model path\\n    parser.add_argument(\\'--nl_query\\', type=s',\n",
       " 'tr, default=\\'a for loop that prints the world\\')\\n    args = parser.parse_args()\\n    \\n    temp_dir = r\"C:\\\\Users\\\\agianolini\\\\OneDrive - ANDES WEALTH MANAGEMENT SA\\\\Desktop\\\\research-assistant-main\\\\output\" # Temporary directory to clone the repo\\n    clone_github_',\n",
       " 'repo(args.github_url, temp_dir)\\n\\n    # Get the project structure\\n    project_structure = get_project_structure(temp_dir)\\n\\n    # Save the project structure to a JSON file\\n    os.makedirs(args.output_dir_path, exist_ok=True)\\n    project_name = os.path.basena',\n",
       " \"me(args.github_url)\\n    output_path = os.path.join(args.output_dir_path, f'{project_name}.json')\\n    with open(output_path, 'w', encoding='utf-8') as f:\\n        json.dump(project_structure, f, ensure_ascii=False, indent=4)\\n    \\n    data = {}\\n    # Leer y a\",\n",
       " 'nalizar el archivo JSON\\n    try:\\n       with open(output_path, \\'r\\') as file:\\n        data = json.load(file)\\n    except Exception as e:\\n        print(f\"Error al leer el archivo JSON: {e}\")\\n\\n    # Proceso de extracción de fragmentos de código\\n    code_snippe',\n",
       " \"ts = []\\n    try:\\n    # Iterar sobre todas las claves principales del diccionario JSON\\n        for main_key in data.keys():\\n            for filename, content in data[main_key].items():\\n                if 'functions' in content:\\n                    for funct\",\n",
       " 'ion_name, function_content in content[\\'functions\\'].items():\\n                        if \\'code\\' in function_content:\\n                            code_snippets.append(function_content[\\'code\\'])\\n    except Exception as e:\\n        code_snippets = f\"Error al proc',\n",
       " 'esar el JSON: {e}\"\\n\\n    searcher = CodeSearcher(args.model_path, code_snippets)\\n    k=3\\n    t = searcher.get_similarity_search(args.nl_query, k)\\n\\n    print(\"Top K similar items: \\\\n\")\\n    print(t)',\n",
       " \"def main():\\n    parser = argparse.ArgumentParser(description='Parse a project structure into a JSON representation.')\\n    parser.add_argument('--project_path', type=str, required=True, help='Path to the root of the project.')\\n    parser.add_argument('--out\",\n",
       " 'put_dir_path\\', type=str, required=True, help=\\'Path to the output dir.\\')\\n    parser.add_argument(\\'--model_path\\', type=str, default= r\"C:\\\\Users\\\\agianolini\\\\OneDrive - ANDES WEALTH MANAGEMENT SA\\\\Desktop\\\\research-assistant-main\\\\model (7).bin\") # change to your ',\n",
       " \"model path\\n    parser.add_argument('--nl_query', type=str, default='a for loop that prints the world')\\n    args = parser.parse_args()\\n    \\n    # Get the project structure\\n    project_structure = get_project_structure(args.project_path)\\n\\n    # Save the proj\",\n",
       " \"ect structure to a JSON file\\n    os.makedirs(args.output_dir_path, exist_ok=True)\\n    project_name = os.path.basename(args.project_path)\\n    output_path = os.path.join(args.output_dir_path, f'{project_name}.json')\\n    with open(output_path, 'w', encoding='\",\n",
       " \"utf-8') as f:\\n        json.dump(project_structure, f, ensure_ascii=False, indent=4)\\n    \\n    data = {}\\n    # Leer y analizar el archivo JSON\\n    try:\\n       with open(output_path, 'r') as file:\\n        data = json.load(file)\\n    except Exception as e:\\n    \",\n",
       " '    print(f\"Error al leer el archivo JSON: {e}\")\\n\\n    # Proceso de extracción de fragmentos de código\\n    code_snippets = []\\n    try:\\n    # Iterar sobre todas las claves principales del diccionario JSON\\n        for main_key in data.keys():\\n            for ',\n",
       " \"filename, content in data[main_key].items():\\n                if 'functions' in content:\\n                    for function_name, function_content in content['functions'].items():\\n                        if 'code' in function_content:\\n                        \",\n",
       " '    code_snippets.append(function_content[\\'code\\'])\\n    except Exception as e:\\n        code_snippets = f\"Error al procesar el JSON: {e}\"\\n\\n    searcher = CodeSearcher(args.model_path, code_snippets)\\n    k=3\\n    t = searcher.get_similarity_search(args.nl_quer',\n",
       " 'y, k)\\n\\n    print(\"Top K similar items: \\\\n\")\\n    print(t)',\n",
       " 'def get_toc(doc):\\n    toc = doc.get_toc()\\n    json_toc = []\\n    for t in toc:\\n        json_toc.append({\"lvl\": t[0], \"title\": t[1], \"page\": t[2]})\\n    page_ranges = { json_toc[i][\\'title\\']: (json_toc[i][\\'page\\'], json_toc[i+1][\\'page\\']) for i in  range(len(jso',\n",
       " 'n_toc)-1) }\\n    return page_ranges',\n",
       " 'def extract_text(doc, page):\\n    text = \"\"\\n    for page in doc:\\n        text += page.getText()\\n    return text',\n",
       " 'def extract_pages(doc, start, end):\\n    res = []\\n    start = max (int(start), 0)\\n    end = min (int(end), len(doc))\\n\\n    for i in range(start, end+1):\\n        res.append(doc[i].get_text(\"text\").strip())\\n    return res',\n",
       " 'def get_args():\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\"pdf_file\", help=\"The pdf file to be processed\")\\n    parser.add_argument(\"operation\", help=\"The operation to be performed\")\\n    parser.add_argument(\"start\",help=\"The starting pa',\n",
       " 'ge\")\\n    parser.add_argument(\"end\",help=\"The ending page\")\\n\\n    args = parser.parse_args()\\n    return args']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_code_snippets(prueba_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "\n",
    "def clone_github_repo(github_url, dest_folder):\n",
    "    \"\"\"\n",
    "    Clone a GitHub repository to the specified local directory, even if the directory is not empty.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a temporary directory to clone the repo\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Clone the repository into the temporary directory\n",
    "            subprocess.run([\"git\", \"clone\", github_url, temp_dir], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            \n",
    "            # Move the contents of the cloned repo to the destination folder\n",
    "            repo_name = os.path.basename(github_url.rstrip('/').split('/')[-1])\n",
    "            source_folder = os.path.join(temp_dir, repo_name)\n",
    "            for item in os.listdir(source_folder):\n",
    "                s = os.path.join(source_folder, item)\n",
    "                d = os.path.join(dest_folder, item)\n",
    "                if os.path.isdir(s):\n",
    "                    shutil.copytree(s, d, dirs_exist_ok=True)\n",
    "                else:\n",
    "                    shutil.copy2(s, d)\n",
    "            \n",
    "            print(\"Repository cloned successfully.\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error in subprocess: {e.stderr.decode().strip()}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_searcher import CodeSearcher, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_snippets(json_data, max_length=256):\n",
    "    code_snippets = []\n",
    "\n",
    "    def add_snippet(code):\n",
    "        for i in range(0, len(code), max_length):\n",
    "            code_snippets.append(code[i:i + max_length])\n",
    "\n",
    "    def process_dict(a_dict):\n",
    "        for key, value in a_dict.items():\n",
    "            if isinstance(value, dict):\n",
    "                if 'code' in value:\n",
    "                    add_snippet(value['code'])\n",
    "                else:\n",
    "                    process_dict(value)\n",
    "\n",
    "    process_dict(json_data)\n",
    "    return code_snippets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlsplit\n",
    "\n",
    "def get_last_segment(url):\n",
    "    \"\"\"\n",
    "    Extracts the last part (segment) of a URL's path.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL to extract the last segment from.\n",
    "\n",
    "    Returns:\n",
    "    str: The last segment of the URL's path.\n",
    "    \"\"\"\n",
    "    parsed_url = urlsplit(url)\n",
    "    path_segments = parsed_url.path.split('/')\n",
    "    last_segment = path_segments[-1]\n",
    "    return last_segment\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://www.example.com/path/to/resource\"\n",
    "last_segment = get_last_segment(url)\n",
    "print(last_segment)  # Output: \"resource\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository cloned successfully.\n",
      "Top K similar items: \n",
      "\n",
      "{'top_3': {'index': 34, 'snippet': 'class NormalizedSoftAdapt(SoftAdaptBase):\\n    \"\"\"The normalized-slopes variant class.\\n\\n    The normalized variant of SoftAdapt is described in section 3.1.3 of our\\n    manuscript (located at: https://arxiv.org/pdf/1912.12355.pdf).\\n\\n    Attributes:\\n        ', 'similarity': 0.6222495311109397}, 'top_2': {'index': 55, 'snippet': 'class SoftAdapt(SoftAdaptBase):\\n    \"\"\"The original variant class.\\n\\n    The original variant of SoftAdapt is described in section 3.1.1 of our\\n    manuscript (located at: https://arxiv.org/pdf/1912.12355.pdf).\\n\\n    Attributes:\\n        beta: A float that is', 'similarity': 0.6070806460289828}, 'top_1': {'index': 11, 'snippet': 'class LossWeightedSoftAdapt(SoftAdaptBase):\\n    \"\"\"Class implementation of the loss-weighted SoftAdapt variant.\\n\\n    The loss-weighted variant of SoftAdapt is described in section 3.1.1 of our\\n    manuscript (located at: https://arxiv.org/pdf/1912.12355.pd', 'similarity': 0.5809575781018815}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "github_url = \"https://github.com/dr-aheydari/SoftAdapt\"\n",
    "output_dir_path = r\"C:\\Users\\agianolini\\OneDrive - ANDES WEALTH MANAGEMENT SA\\Desktop\\research-assistant-main\"\n",
    "model_path = r\"C:\\Users\\agianolini\\OneDrive - ANDES WEALTH MANAGEMENT SA\\Desktop\\research-assistant-main\\model (7).bin\"\n",
    "nl_query = 'importing softadapt'\n",
    "\n",
    "\n",
    "\n",
    "temp_dir = r\"C:\\Users\\agianolini\\OneDrive - ANDES WEALTH MANAGEMENT SA\\Desktop\\research-assistant-main\\output\" # Temporary directory to clone the repo\n",
    "clone_github_repo(github_url, temp_dir)\n",
    "repo_name = get_last_segment(github_url)\n",
    "\n",
    "# Get the project structure\n",
    "project_structure = get_project_structure(temp_dir )\n",
    "\n",
    "# Save the project structure to a JSON file\n",
    "os.makedirs(output_dir_path, exist_ok=True)\n",
    "project_name = os.path.basename(github_url)\n",
    "output_path = os.path.join(output_dir_path, f'{project_name}.json')\n",
    "\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(project_structure, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "data = {}\n",
    "# Leer y analizar el archivo JSON\n",
    "try:\n",
    "    with open(output_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer el archivo JSON: {e}\")\n",
    "\n",
    "# Proceso de extracción de fragmentos de código\n",
    "code_snippets = []\n",
    "code_snippets = extract_code_snippets(data)\n",
    "# try:\n",
    "# # Iterar sobre todas las claves principales del diccionario JSON\n",
    "#     for main_key in data.keys():\n",
    "#         for filename, content in data[main_key].items():\n",
    "#             if 'functions' in content:\n",
    "#                 for function_name, function_content in content['functions'].items():\n",
    "#                     if 'code' in function_content:\n",
    "#                         code_snippets.append(function_content['code'])\n",
    "# except Exception as e:\n",
    "#     code_snippets = f\"Error al procesar el JSON: {e}\"\n",
    "\n",
    "searcher = CodeSearcher(model_path, code_snippets)\n",
    "k=3\n",
    "t = searcher.get_similarity_search(nl_query, k)\n",
    "\n",
    "print(\"Top K similar items: \\n\")\n",
    "print(t)\n",
    "\n",
    "# print(data)\n",
    "# print(code_snippets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top_3': {'index': 34,\n",
       "  'snippet': 'class NormalizedSoftAdapt(SoftAdaptBase):\\n    \"\"\"The normalized-slopes variant class.\\n\\n    The normalized variant of SoftAdapt is described in section 3.1.3 of our\\n    manuscript (located at: https://arxiv.org/pdf/1912.12355.pdf).\\n\\n    Attributes:\\n        ',\n",
       "  'similarity': 0.6222495311109397},\n",
       " 'top_2': {'index': 55,\n",
       "  'snippet': 'class SoftAdapt(SoftAdaptBase):\\n    \"\"\"The original variant class.\\n\\n    The original variant of SoftAdapt is described in section 3.1.1 of our\\n    manuscript (located at: https://arxiv.org/pdf/1912.12355.pdf).\\n\\n    Attributes:\\n        beta: A float that is',\n",
       "  'similarity': 0.6070806460289828},\n",
       " 'top_1': {'index': 11,\n",
       "  'snippet': 'class LossWeightedSoftAdapt(SoftAdaptBase):\\n    \"\"\"Class implementation of the loss-weighted SoftAdapt variant.\\n\\n    The loss-weighted variant of SoftAdapt is described in section 3.1.1 of our\\n    manuscript (located at: https://arxiv.org/pdf/1912.12355.pd',\n",
       "  'similarity': 0.5809575781018815}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
