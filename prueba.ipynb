{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_code import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba_json = get_project_structure(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_snippets(json_data, max_length=256):\n",
    "    code_snippets = []\n",
    "    for main_key, main_value in json_data.items():\n",
    "        if isinstance(main_value, dict):  # Check if the value is a dictionary\n",
    "            for sub_key, sub_value in main_value.items():\n",
    "                if isinstance(sub_value, dict):\n",
    "                    functions = sub_value.get('functions')\n",
    "                    if functions:\n",
    "                        for func_name, func_details in functions.items():\n",
    "                            code = func_details.get('code')\n",
    "                            if code:\n",
    "                                # Split the code into chunks of max_length\n",
    "                                for i in range(0, len(code), max_length):\n",
    "                                    snippet = code[i:i + max_length]\n",
    "                                    code_snippets.append(snippet)\n",
    "    return code_snippets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    def __init__(self, model_path, code_snippets, device=None):\\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\\n        self.tokenizer = RobertaTokenizer.from_pretrained(\\'microsoft/unixcoder-',\n",
       " \"base')\\n        self.model = self.load_model(model_path)\\n        self.code_snippets = code_snippets\\n        self.code_embeddings = self.generate_code_embeddings(code_snippets)\",\n",
       " '    def forward(self, code_inputs=None, nl_inputs=None):\\n        if code_inputs is not None:\\n            outputs = self.encoder(code_inputs,attention_mask=code_inputs.ne(1))[0]\\n            outputs = (outputs*code_inputs.ne(1)[:,:,None]).sum(1)/code_inputs.',\n",
       " 'ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)\\n        else:\\n            outputs = self.encoder(nl_inputs,attention_mask=nl_inputs.ne(1))[0]\\n            outputs = (outputs*nl_inputs.ne(1)[:,:,None]).sum(1)/nl_in',\n",
       " 'puts.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)',\n",
       " \"    def load_model(self, model_path):\\n        model = RobertaModel.from_pretrained('microsoft/unixcoder-base')\\n        model = Model(model)\\n        model.load_state_dict(torch.load(model_path, map_location=self.device))\\n        model.to(self.device)\\n      \",\n",
       " '  model.eval()\\n        return model',\n",
       " \"    def generate_code_embeddings(self, code_snippets):\\n        embeddings = []\\n        for snippet in code_snippets:\\n            inputs = self.tokenizer.encode_plus(snippet, add_special_tokens=True, max_length=256, truncation=True, padding='max_length', re\",\n",
       " \"turn_tensors='pt')\\n            with torch.no_grad():\\n                embedding = self.model(code_inputs=inputs['input_ids'].to(self.device))\\n            embeddings.append(embedding.cpu().numpy())\\n        return np.vstack(embeddings)\",\n",
       " \"    def get_query_embedding(self, query):\\n        inputs = self.tokenizer.encode_plus(query, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\\n        with torch.no_grad():\\n            embedding = self.mod\",\n",
       " \"el(code_inputs=inputs['input_ids'].to(self.device))\\n        return embedding.cpu().numpy()\",\n",
       " \"    def get_similarity_search(self, query, k):\\n        query_embedding = self.get_query_embedding(query)\\n        similarities = 1 - cdist(query_embedding, self.code_embeddings, 'cosine').flatten()\\n\\n        # Get top-k indices\\n        top_k_indices = np.arg\",\n",
       " \"sort(similarities)[-k:]\\n\\n        # Create a dictionary for the top k results\\n        results = {}\\n        for index in reversed(top_k_indices):\\n            snippet_info = {\\n                'index': index,\\n                'snippet': self.code_snippets[index\",\n",
       " \"],\\n                'similarity': similarities[index]\\n            }\\n            results[f'top_{k}'] = snippet_info\\n            k -= 1\\n    \\n        return results\",\n",
       " '    def __init__(self, model_path, code_snippets, device=None):\\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\\n        self.tokenizer = RobertaTokenizer.from_pretrained(\\'microsoft/unixcoder-',\n",
       " \"base')\\n        self.model = self.load_model(model_path)\\n        self.code_snippets = code_snippets\\n        self.code_embeddings = self.generate_code_embeddings(code_snippets)\",\n",
       " '    def forward(self, code_inputs=None, nl_inputs=None):\\n        if code_inputs is not None:\\n            outputs = self.encoder(code_inputs,attention_mask=code_inputs.ne(1))[0]\\n            outputs = (outputs*code_inputs.ne(1)[:,:,None]).sum(1)/code_inputs.',\n",
       " 'ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)\\n        else:\\n            outputs = self.encoder(nl_inputs,attention_mask=nl_inputs.ne(1))[0]\\n            outputs = (outputs*nl_inputs.ne(1)[:,:,None]).sum(1)/nl_in',\n",
       " 'puts.ne(1).sum(-1)[:,None]\\n            return torch.nn.functional.normalize(outputs, p=2, dim=1)',\n",
       " \"    def load_model(self, model_path):\\n        model = RobertaModel.from_pretrained('microsoft/unixcoder-base')\\n        model = Model(model)\\n        model.load_state_dict(torch.load(model_path, map_location=self.device))\\n        model.to(self.device)\\n      \",\n",
       " '  model.eval()\\n        return model',\n",
       " \"    def generate_code_embeddings(self, code_snippets):\\n        embeddings = []\\n        for snippet in code_snippets:\\n            inputs = self.tokenizer.encode_plus(snippet, add_special_tokens=True, max_length=256, truncation=True, padding='max_length', re\",\n",
       " \"turn_tensors='pt')\\n            with torch.no_grad():\\n                embedding = self.model(code_inputs=inputs['input_ids'].to(self.device))\\n            embeddings.append(embedding.cpu().numpy())\\n        return np.vstack(embeddings)\",\n",
       " \"    def get_query_embedding(self, query):\\n        inputs = self.tokenizer.encode_plus(query, add_special_tokens=True, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\\n        with torch.no_grad():\\n            embedding = self.mod\",\n",
       " \"el(code_inputs=inputs['input_ids'].to(self.device))\\n        return embedding.cpu().numpy()\",\n",
       " \"    def get_similarity_search(self, query, k):\\n        query_embedding = self.get_query_embedding(query)\\n        similarities = 1 - cdist(query_embedding, self.code_embeddings, 'cosine').flatten()\\n\\n        # Get top-k indices\\n        top_k_indices = np.arg\",\n",
       " \"sort(similarities)[-k:]\\n\\n        # Create a dictionary for the top k results\\n        results = {}\\n        for index in reversed(top_k_indices):\\n            snippet_info = {\\n                'index': index,\\n                'snippet': self.code_snippets[index\",\n",
       " \"],\\n                'similarity': similarities[index]\\n            }\\n            results[f'top_{k}'] = snippet_info\\n            k -= 1\\n\\n        return results\",\n",
       " 'def code_to_json(code_lines, tree):\\n    res = {}\\n    functions = {}\\n    classes = {}\\n    direct_imports = {}\\n    from_imports = defaultdict(list)\\n\\n    # Extract module-level docstring, if present\\n    module_docstring = ast.get_docstring(tree)\\n\\n    for node',\n",
       " ' in ast.walk(tree):\\n        if isinstance(node, ast.FunctionDef):\\n            function_code = \"\\\\n\".join(code_lines[node.lineno - 1: node.end_lineno])\\n            function_lines = {\"start\": node.lineno - 1, \"end\": node.end_lineno}\\n            function_docst',\n",
       " 'ring = ast.get_docstring(node)\\n            functions[node.name] = {\"code\": function_code, \"lines\": function_lines, \"docstring\": function_docstring}\\n        elif isinstance(node, ast.ClassDef):\\n            class_code = \"\\\\n\".join(code_lines[node.lineno - 1: ',\n",
       " 'node.end_lineno])\\n            class_lines = {\"start\": node.lineno - 1, \"end\": node.end_lineno}\\n            class_docstring = ast.get_docstring(node)\\n            classes[node.name] = {\"code\": class_code, \"lines\": class_lines, \"docstring\": class_docstring}\\n ',\n",
       " '       elif isinstance(node, ast.Import):\\n            for alias in node.names:\\n                direct_imports[alias.name] = {\"alias\": alias.asname, \"lineno\": node.lineno}\\n        elif isinstance(node, ast.ImportFrom):\\n            for alias in node.names:\\n ',\n",
       " '               from_imports[node.module].append({\\n                    \"name\": alias.name,\\n                    \"alias\": alias.asname,\\n                    \"lineno\": node.lineno\\n                })\\n\\n    res[\"functions\"] = functions\\n    res[\"classes\"] = classes',\n",
       " '\\n    res[\"direct_imports\"] = direct_imports\\n    res[\"from_imports\"] = dict(from_imports)  # Convert back to a regular dict for JSON serialization\\n    res[\"module_docstring\"] = module_docstring\\n\\n    return res',\n",
       " \"def get_file_code(file_path):\\n    with open(file_path, 'r', encoding='utf-8') as f:\\n        if file_path.endswith('.py'):\\n            code = f.read()\\n            code_lines = code.split('\\\\n')\\n            tree = ast.parse(code)\\n            res = code_to_jso\",\n",
       " \"n(code_lines, tree)\\n        elif file_path.endswith('.ipynb'):\\n            notebook = json.load(f)\\n            res = {'cells': [cell['source'] for cell in notebook['cells'] if cell['cell_type'] == 'code']}\\n    return res\",\n",
       " \"def parse_md(file_path):\\n    with open(file_path, 'r', encoding='utf-8') as f:\\n        return markdown.markdown(f.read())\",\n",
       " \"def get_project_structure(project_path):\\n    res = {}\\n    file_path = None\\n\\n    project_name = os.path.basename(project_path.rstrip(os.sep)) # Get the project folder name\\n\\n    for root, dirs, files in os.walk(project_path):\\n        # Skip if it's before th\",\n",
       " 'e project directory\\n        if project_name not in root:\\n            continue\\n\\n        path_parts = root.split(os.sep)\\n        # Start from the project directory in the path_parts\\n        start_index = path_parts.index(project_name)\\n        relevant_path_p',\n",
       " 'arts = path_parts[start_index:]\\n\\n        current_level = res\\n        for part in relevant_path_parts:\\n            if part not in current_level:\\n                current_level[part] = {}\\n            current_level = current_level[part]\\n        structure = cur',\n",
       " 'rent_level\\n\\n        for directory in dirs:\\n            structure[directory] = {}\\n\\n        for file in files:\\n            try:\\n                file_path = os.path.join(root, file)\\n                if file.endswith(\".py\") or file.endswith(\".ipynb\"):\\n         ',\n",
       " '           structure[file] = get_file_code(file_path)\\n                elif file.endswith(\".md\"):\\n                    structure[file] = parse_md(file_path)\\n            except Exception as e:\\n                print(f\"Failed to process file {file_path}: {e}\")\\n',\n",
       " '\\n    return res',\n",
       " \"def summarize_structure(project_structure, indent=''):\\n    python_files = 0\\n    ipynb_files = 0\\n    md_files = 0\\n    function_counts = []\\n    class_counts = []\\n    import_counts = []\\n\\n    for key, value in project_structure.items():\\n        if isinstance(v\",\n",
       " \"alue, dict):\\n            if key.endswith('.py') or key.endswith('.ipynb'):\\n                python_files += 1\\n                function_counts.append(len(value.get('functions', {})))\\n                class_counts.append(len(value.get('classes', {})))\\n        \",\n",
       " \"        import_counts.append(len(value.get('imports', {})))\\n            elif key.endswith('.md'):\\n                md_files += 1\\n            else:\\n                # recurse into subdirectories\\n                sub_python_files, sub_ipynb_files, sub_md_files,\",\n",
       " \" sub_function_counts, sub_class_counts, sub_import_counts = summarize_structure(\\n                    value, indent + '  ')\\n                python_files += sub_python_files\\n                ipynb_files += sub_ipynb_files\\n                md_files += sub_md_fi\",\n",
       " 'les\\n                function_counts.extend(sub_function_counts)\\n                class_counts.extend(sub_class_counts)\\n                import_counts.extend(sub_import_counts)\\n\\n    return python_files, ipynb_files, md_files, function_counts, class_counts, im',\n",
       " 'port_counts',\n",
       " 'def get_imports(tree):\\n    imports = set()\\n\\n    for node in ast.walk(tree):\\n        if isinstance(node, ast.Import):\\n            for alias in node.names:\\n                imports.add(alias.name)\\n        elif isinstance(node, ast.ImportFrom):\\n            if ',\n",
       " \"node.level > 0:\\n                # We have a relative import, which we'll ignore for now\\n                continue\\n\\n            module = node.module\\n            for alias in node.names:\\n                if module is None:\\n                    imports.add(alias\",\n",
       " '.name)\\n                else:\\n                    imports.add(f\"{module}.{alias.name}\")\\n\\n    return imports',\n",
       " \"def analyze_codebase(directory):\\n    results = {}\\n\\n    # Step 2-3: Traverse directory and parse each Python file\\n    for file_path in traverse_directory(directory):\\n        with open(file_path, 'r') as f:\\n            code = f.read()\\n        tree = ast.pars\",\n",
       " 'e(code)\\n        code_lines = code.splitlines()\\n        results[file_path] = code_to_json(code_lines, tree)\\n\\n    # Step 4-5: Analyze imports to find dependencies\\n    dependencies = analyze_dependencies(results)\\n\\n    return results, dependencies',\n",
       " 'def traverse_directory(directory):\\n    for root, dirs, files in os.walk(directory):\\n        for file in files:\\n            if file.endswith(\".py\"):\\n                yield os.path.join(root, file)',\n",
       " \"def is_external(module):\\n    try:\\n        spec = importlib.util.find_spec(module)\\n        return spec is not None and 'site-packages' in spec.origin\\n    except ModuleNotFoundError:\\n        return False\",\n",
       " \"def analyze_dependencies(results):\\n    dependencies = {file: set() for file in results.keys()}\\n\\n    for file, structure in results.items():\\n        if 'direct_imports' in structure:\\n            for imported_module in structure['direct_imports']:\\n          \",\n",
       " '      if not is_external(imported_module):\\n                    # Note: This code assumes the imported file is in the same directory\\n                    # This might not be the case in your project.\\n                    dependencies[file].add(imported_module',\n",
       " ')\\n\\n    return dependencies',\n",
       " 'def process_atlas_data(atlas_data):\\n    # Placeholder for the documents\\n    documents = []\\n\\n    # Iterate over the files in the atlas data\\n    for file_path, file_data in atlas_data.items():\\n        # Check if the file data is a dictionary\\n        if isins',\n",
       " 'tance(file_data, dict):\\n            # Process the functions\\n            for function_name, function_data in file_data.get(\\'functions\\', {}).items():\\n                # Construct the document ID\\n                document_id = f\"{file_path}/function/{function_n',\n",
       " 'ame}\"\\n\\n                # Prepare the document content and metadata\\n                document_content = json.dumps(function_data)\\n                document_metadata = {\\'file_path\\': file_path, \\'type\\': \\'function\\', \\'name\\': function_name}\\n\\n                # Add t',\n",
       " \"he document to the list\\n                documents.append((document_id, document_content, document_metadata))\\n\\n            # Process the classes\\n            for class_name, class_data in file_data.get('classes', {}).items():\\n                # Construct the \",\n",
       " 'document ID\\n                document_id = f\"{file_path}/class/{class_name}\"\\n\\n                # Prepare the document content and metadata\\n                document_content = json.dumps(class_data)\\n                document_metadata = {\\'file_path\\': file_path, ',\n",
       " \"'type': 'class', 'name': class_name}\\n\\n                # Add the document to the list\\n                documents.append((document_id, document_content, document_metadata))\\n\\n    return documents\",\n",
       " \"def get_project_structure(project_path):\\n    res = {}\\n    repo_name = os.path.basename(project_path)\\n\\n    for root, dirs, files in os.walk(project_path):\\n        # Trim the root to start from the repository's root directory\\n        trimmed_root = root[len(\",\n",
       " 'project_path):].lstrip(os.sep)\\n        path_parts = trimmed_root.split(os.sep)\\n        current_level = res\\n\\n        # Construct the nested dictionary structure\\n        for part in path_parts:\\n            current_level = current_level.setdefault(part, {})\\n\\n',\n",
       " '        for file in files:\\n            try:\\n                file_path = os.path.join(root, file)\\n                if file.endswith(\".py\") or file.endswith(\".ipynb\"):\\n                    current_level[file] = get_file_code(file_path)\\n                elif fil',\n",
       " 'e.endswith(\".md\"):\\n                    current_level[file] = parse_md(file_path)\\n            except Exception as e:\\n                print(f\"Failed to process file {file_path}: {e}\")\\n\\n    # Wrap in the repository\\'s name\\n    return {repo_name: res}',\n",
       " 'def clone_github_repo(github_url, dest_folder):\\n    \"\"\"\\n    Clone a GitHub repository to the specified local directory, even if the directory is not empty.\\n    \"\"\"\\n    try:\\n        # Create a temporary directory to clone the repo\\n        with tempfile.Temp',\n",
       " 'oraryDirectory() as temp_dir:\\n            # Clone the repository into the temporary directory\\n            subprocess.run([\"git\", \"clone\", github_url, temp_dir], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\\n            \\n            # Move the',\n",
       " \" contents of the cloned repo to the destination folder\\n            repo_name = os.path.basename(github_url.rstrip('/').split('/')[-1])\\n            source_folder = os.path.join(temp_dir, repo_name)\\n            for item in os.listdir(source_folder):\\n        \",\n",
       " '        s = os.path.join(source_folder, item)\\n                d = os.path.join(dest_folder, item)\\n                if os.path.isdir(s):\\n                    shutil.copytree(s, d, dirs_exist_ok=True)\\n                else:\\n                    shutil.copy2(s, d',\n",
       " ')\\n            \\n            print(\"Repository cloned successfully.\")\\n\\n    except subprocess.CalledProcessError as e:\\n        print(f\"Error in subprocess: {e.stderr.decode().strip()}\")\\n        raise\\n    except Exception as e:\\n        print(f\"Error: {e}\")\\n   ',\n",
       " '     raise',\n",
       " \"def main():\\n    parser = argparse.ArgumentParser(description='Parse a project structure into a JSON representation.')\\n    # parser.add_argument('--github_url', type=str, required=True, help='URL to the GitHub repository.')    \\n    parser.add_argument('--gi\",\n",
       " 'thub_url\\', type=str, default= \"https://github.com/dr-aheydari/SoftAdapt\", help=\\'URL to the GitHub repository.\\')    \\n    # parser.add_argument(\\'--pdf_path\\', type=str, required=True, help=\\'Path to the paper PDF\\')\\n    # parser.add_argument(\\'--output_dir_path\\'',\n",
       " ', type=str, required=True, help=\\'Path to the output dir.\\')\\n    parser.add_argument(\\'--output_dir_path\\', type=str, default= r\"C:\\\\Users\\\\agianolini\\\\OneDrive - ANDES WEALTH MANAGEMENT SA\\\\Desktop\\\\research-assistant-main\", \\n                        help=\\'Path to ',\n",
       " 'the output dir.\\')\\n    parser.add_argument(\\'--model_path\\', type=str, default=r\"C:\\\\Users\\\\agianolini\\\\OneDrive - ANDES WEALTH MANAGEMENT SA\\\\Desktop\\\\research-assistant-main\\\\model (7).bin\") # change to your model path\\n    parser.add_argument(\\'--nl_query\\', type=s',\n",
       " 'tr, default=\\'a for loop that prints the world\\')\\n    args = parser.parse_args()\\n    \\n    temp_dir = r\"C:\\\\Users\\\\agianolini\\\\OneDrive - ANDES WEALTH MANAGEMENT SA\\\\Desktop\\\\research-assistant-main\\\\output\" # Temporary directory to clone the repo\\n    clone_github_',\n",
       " 'repo(args.github_url, temp_dir)\\n\\n    # Get the project structure\\n    project_structure = get_project_structure(temp_dir)\\n\\n    # Save the project structure to a JSON file\\n    os.makedirs(args.output_dir_path, exist_ok=True)\\n    project_name = os.path.basena',\n",
       " \"me(args.github_url)\\n    output_path = os.path.join(args.output_dir_path, f'{project_name}.json')\\n    with open(output_path, 'w', encoding='utf-8') as f:\\n        json.dump(project_structure, f, ensure_ascii=False, indent=4)\\n    \\n    data = {}\\n    # Leer y a\",\n",
       " 'nalizar el archivo JSON\\n    try:\\n       with open(output_path, \\'r\\') as file:\\n        data = json.load(file)\\n    except Exception as e:\\n        print(f\"Error al leer el archivo JSON: {e}\")\\n\\n    # Proceso de extracción de fragmentos de código\\n    code_snippe',\n",
       " \"ts = []\\n    try:\\n    # Iterar sobre todas las claves principales del diccionario JSON\\n        for main_key in data.keys():\\n            for filename, content in data[main_key].items():\\n                if 'functions' in content:\\n                    for funct\",\n",
       " 'ion_name, function_content in content[\\'functions\\'].items():\\n                        if \\'code\\' in function_content:\\n                            code_snippets.append(function_content[\\'code\\'])\\n    except Exception as e:\\n        code_snippets = f\"Error al proc',\n",
       " 'esar el JSON: {e}\"\\n\\n    searcher = CodeSearcher(args.model_path, code_snippets)\\n    k=3\\n    t = searcher.get_similarity_search(args.nl_query, k)\\n\\n    print(\"Top K similar items: \\\\n\")\\n    print(t)',\n",
       " \"def main():\\n    parser = argparse.ArgumentParser(description='Parse a project structure into a JSON representation.')\\n    parser.add_argument('--project_path', type=str, required=True, help='Path to the root of the project.')\\n    parser.add_argument('--out\",\n",
       " 'put_dir_path\\', type=str, required=True, help=\\'Path to the output dir.\\')\\n    parser.add_argument(\\'--model_path\\', type=str, default= r\"C:\\\\Users\\\\agianolini\\\\OneDrive - ANDES WEALTH MANAGEMENT SA\\\\Desktop\\\\research-assistant-main\\\\model (7).bin\") # change to your ',\n",
       " \"model path\\n    parser.add_argument('--nl_query', type=str, default='a for loop that prints the world')\\n    args = parser.parse_args()\\n    \\n    # Get the project structure\\n    project_structure = get_project_structure(args.project_path)\\n\\n    # Save the proj\",\n",
       " \"ect structure to a JSON file\\n    os.makedirs(args.output_dir_path, exist_ok=True)\\n    project_name = os.path.basename(args.project_path)\\n    output_path = os.path.join(args.output_dir_path, f'{project_name}.json')\\n    with open(output_path, 'w', encoding='\",\n",
       " \"utf-8') as f:\\n        json.dump(project_structure, f, ensure_ascii=False, indent=4)\\n    \\n    data = {}\\n    # Leer y analizar el archivo JSON\\n    try:\\n       with open(output_path, 'r') as file:\\n        data = json.load(file)\\n    except Exception as e:\\n    \",\n",
       " '    print(f\"Error al leer el archivo JSON: {e}\")\\n\\n    # Proceso de extracción de fragmentos de código\\n    code_snippets = []\\n    try:\\n    # Iterar sobre todas las claves principales del diccionario JSON\\n        for main_key in data.keys():\\n            for ',\n",
       " \"filename, content in data[main_key].items():\\n                if 'functions' in content:\\n                    for function_name, function_content in content['functions'].items():\\n                        if 'code' in function_content:\\n                        \",\n",
       " '    code_snippets.append(function_content[\\'code\\'])\\n    except Exception as e:\\n        code_snippets = f\"Error al procesar el JSON: {e}\"\\n\\n    searcher = CodeSearcher(args.model_path, code_snippets)\\n    k=3\\n    t = searcher.get_similarity_search(args.nl_quer',\n",
       " 'y, k)\\n\\n    print(\"Top K similar items: \\\\n\")\\n    print(t)',\n",
       " 'def get_toc(doc):\\n    toc = doc.get_toc()\\n    json_toc = []\\n    for t in toc:\\n        json_toc.append({\"lvl\": t[0], \"title\": t[1], \"page\": t[2]})\\n    page_ranges = { json_toc[i][\\'title\\']: (json_toc[i][\\'page\\'], json_toc[i+1][\\'page\\']) for i in  range(len(jso',\n",
       " 'n_toc)-1) }\\n    return page_ranges',\n",
       " 'def extract_text(doc, page):\\n    text = \"\"\\n    for page in doc:\\n        text += page.getText()\\n    return text',\n",
       " 'def extract_pages(doc, start, end):\\n    res = []\\n    start = max (int(start), 0)\\n    end = min (int(end), len(doc))\\n\\n    for i in range(start, end+1):\\n        res.append(doc[i].get_text(\"text\").strip())\\n    return res',\n",
       " 'def get_args():\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\"pdf_file\", help=\"The pdf file to be processed\")\\n    parser.add_argument(\"operation\", help=\"The operation to be performed\")\\n    parser.add_argument(\"start\",help=\"The starting pa',\n",
       " 'ge\")\\n    parser.add_argument(\"end\",help=\"The ending page\")\\n\\n    args = parser.parse_args()\\n    return args']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_code_snippets(prueba_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "\n",
    "def clone_github_repo(github_url, dest_folder):\n",
    "    \"\"\"\n",
    "    Clone a GitHub repository to the specified local directory, even if the directory is not empty.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a temporary directory to clone the repo\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Clone the repository into the temporary directory\n",
    "            subprocess.run([\"git\", \"clone\", github_url, temp_dir], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            \n",
    "            # Move the contents of the cloned repo to the destination folder\n",
    "            repo_name = os.path.basename(github_url.rstrip('/').split('/')[-1])\n",
    "            source_folder = os.path.join(temp_dir, repo_name)\n",
    "            for item in os.listdir(source_folder):\n",
    "                s = os.path.join(source_folder, item)\n",
    "                d = os.path.join(dest_folder, item)\n",
    "                if os.path.isdir(s):\n",
    "                    shutil.copytree(s, d, dirs_exist_ok=True)\n",
    "                else:\n",
    "                    shutil.copy2(s, d)\n",
    "            \n",
    "            print(\"Repository cloned successfully.\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error in subprocess: {e.stderr.decode().strip()}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_searcher import CodeSearcher, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_snippets(json_data, max_length=256):\n",
    "    code_snippets = []\n",
    "\n",
    "    def add_snippet(code):\n",
    "        for i in range(0, len(code), max_length):\n",
    "            code_snippets.append(code[i:i + max_length])\n",
    "\n",
    "    def process_dict(a_dict):\n",
    "        for key, value in a_dict.items():\n",
    "            if isinstance(value, dict):\n",
    "                if 'code' in value:\n",
    "                    add_snippet(value['code'])\n",
    "                else:\n",
    "                    process_dict(value)\n",
    "\n",
    "    process_dict(json_data)\n",
    "    return code_snippets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlsplit\n",
    "\n",
    "def get_last_segment(url):\n",
    "    \"\"\"\n",
    "    Extracts the last part (segment) of a URL's path.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL to extract the last segment from.\n",
    "\n",
    "    Returns:\n",
    "    str: The last segment of the URL's path.\n",
    "    \"\"\"\n",
    "    parsed_url = urlsplit(url)\n",
    "    path_segments = parsed_url.path.split('/')\n",
    "    last_segment = path_segments[-1]\n",
    "    return last_segment\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://www.example.com/path/to/resource\"\n",
    "last_segment = get_last_segment(url)\n",
    "print(last_segment)  # Output: \"resource\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository cloned successfully.\n",
      "{'output': {'.git': {'hooks': {}, 'info': {}, 'logs': {'refs': {'heads': {}, 'remotes': {'origin': {}}}}, 'objects': {'info': {}, 'pack': {}}, 'refs': {'heads': {}, 'remotes': {'origin': {}}, 'tags': {}}}, 'algorithms': {'loss_weighted_variant.py': {'functions': {'__init__': {'code': '    def __init__(self, beta: int = 0.1, accuracy_order: int = None):\\n        \"\"\"SoftAdapt class initializer.\"\"\"\\n        super().__init__()\\n        self.beta = beta\\n        # Passing \"None\" as the order of accuracy sets the highest possible\\n        # accuracy in the finite difference approximation.\\n        self.accuracy_order = accuracy_order', 'lines': {'start': 24, 'end': 31}, 'docstring': 'SoftAdapt class initializer.'}, 'get_component_weights': {'code': '    def get_component_weights(self,\\n                               *loss_component_values: Tuple[torch.tensor],\\n                               verbose: bool = True):\\n        \"\"\"Class method for SoftAdapt weights.\\n\\n        Args:\\n            loss_component_values: A tuple consisting of the values of the each\\n              loss component that have been stored for the past \\'n\\' iterations\\n              or epochs (as described in the manuscript).\\n            verbose: A boolean indicating user preference for whether internal\\n              functions should print out information and warning about\\n              computations.\\n        Returns:\\n            The computed weights for each loss components. For example, if there\\n            were 5 loss components, say (l_1, l_2, l_3, l_4, l_5), then the\\n            return tensor will be the weights (alpha_1, alpha_2, alpha_3,\\n            alpha_4, alpha_5) in the order of the loss components.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        if len(loss_component_values) == 1:\\n            print(\"==> Warning: You have only passed on the values of one loss\"\\n                  \" component, which will result in trivial weighting.\")\\n\\n        rates_of_change = []\\n        average_loss_values = []\\n\\n        for loss_points in loss_component_values:\\n            # Compute the rates of change for each one of the loss components.\\n            rates_of_change.append(\\n                self._compute_rates_of_change(loss_points,\\n                                              self.accuracy_order,\\n                                              verbose=verbose))\\n            average_loss_values.append(torch.mean(loss_points.float()))\\n\\n        rates_of_change = torch.tensor(rates_of_change)\\n        average_loss_values = torch.tensor(average_loss_values)\\n        # Calculate the weight and return the values.\\n        return self._softmax(input_tensor=rates_of_change,\\n                             beta=self.beta,\\n                             numerator_weights = average_loss_values,\\n                             )', 'lines': {'start': 32, 'end': 76}, 'docstring': \"Class method for SoftAdapt weights.\\n\\nArgs:\\n    loss_component_values: A tuple consisting of the values of the each\\n      loss component that have been stored for the past 'n' iterations\\n      or epochs (as described in the manuscript).\\n    verbose: A boolean indicating user preference for whether internal\\n      functions should print out information and warning about\\n      computations.\\nReturns:\\n    The computed weights for each loss components. For example, if there\\n    were 5 loss components, say (l_1, l_2, l_3, l_4, l_5), then the\\n    return tensor will be the weights (alpha_1, alpha_2, alpha_3,\\n    alpha_4, alpha_5) in the order of the loss components.\\n\\nRaises:\\n    None.\"}}, 'classes': {'LossWeightedSoftAdapt': {'code': 'class LossWeightedSoftAdapt(SoftAdaptBase):\\n    \"\"\"Class implementation of the loss-weighted SoftAdapt variant.\\n\\n    The loss-weighted variant of SoftAdapt is described in section 3.1.1 of our\\n    manuscript (located at: https://arxiv.org/pdf/1912.12355.pdf).\\n\\n    Attributes:\\n        beta: A float that is the \\'beta\\' hyperparameter in our manuscript. If\\n          beta > 0, then softAdapt will pay more attention the worst performing\\n          loss component. If beta < 0, then SoftAdapt will assign higher weights\\n          to the better performing components. Beta==0 is the trivial case and\\n          all loss components will have coefficient 1.\\n\\n        accuracy_order: An integer indicating the accuracy order of the finite\\n          volume approximation of each loss component\\'s slope.\\n    \"\"\"\\n\\n    def __init__(self, beta: int = 0.1, accuracy_order: int = None):\\n        \"\"\"SoftAdapt class initializer.\"\"\"\\n        super().__init__()\\n        self.beta = beta\\n        # Passing \"None\" as the order of accuracy sets the highest possible\\n        # accuracy in the finite difference approximation.\\n        self.accuracy_order = accuracy_order\\n\\n    def get_component_weights(self,\\n                               *loss_component_values: Tuple[torch.tensor],\\n                               verbose: bool = True):\\n        \"\"\"Class method for SoftAdapt weights.\\n\\n        Args:\\n            loss_component_values: A tuple consisting of the values of the each\\n              loss component that have been stored for the past \\'n\\' iterations\\n              or epochs (as described in the manuscript).\\n            verbose: A boolean indicating user preference for whether internal\\n              functions should print out information and warning about\\n              computations.\\n        Returns:\\n            The computed weights for each loss components. For example, if there\\n            were 5 loss components, say (l_1, l_2, l_3, l_4, l_5), then the\\n            return tensor will be the weights (alpha_1, alpha_2, alpha_3,\\n            alpha_4, alpha_5) in the order of the loss components.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        if len(loss_component_values) == 1:\\n            print(\"==> Warning: You have only passed on the values of one loss\"\\n                  \" component, which will result in trivial weighting.\")\\n\\n        rates_of_change = []\\n        average_loss_values = []\\n\\n        for loss_points in loss_component_values:\\n            # Compute the rates of change for each one of the loss components.\\n            rates_of_change.append(\\n                self._compute_rates_of_change(loss_points,\\n                                              self.accuracy_order,\\n                                              verbose=verbose))\\n            average_loss_values.append(torch.mean(loss_points.float()))\\n\\n        rates_of_change = torch.tensor(rates_of_change)\\n        average_loss_values = torch.tensor(average_loss_values)\\n        # Calculate the weight and return the values.\\n        return self._softmax(input_tensor=rates_of_change,\\n                             beta=self.beta,\\n                             numerator_weights = average_loss_values,\\n                             )', 'lines': {'start': 7, 'end': 76}, 'docstring': \"Class implementation of the loss-weighted SoftAdapt variant.\\n\\nThe loss-weighted variant of SoftAdapt is described in section 3.1.1 of our\\nmanuscript (located at: https://arxiv.org/pdf/1912.12355.pdf).\\n\\nAttributes:\\n    beta: A float that is the 'beta' hyperparameter in our manuscript. If\\n      beta > 0, then softAdapt will pay more attention the worst performing\\n      loss component. If beta < 0, then SoftAdapt will assign higher weights\\n      to the better performing components. Beta==0 is the trivial case and\\n      all loss components will have coefficient 1.\\n\\n    accuracy_order: An integer indicating the accuracy order of the finite\\n      volume approximation of each loss component's slope.\"}}, 'direct_imports': {'torch': {'alias': None, 'lineno': 3}}, 'from_imports': {'base._softadapt_base_class': [{'name': 'SoftAdaptBase', 'alias': None, 'lineno': 4}], 'typing': [{'name': 'Tuple', 'alias': None, 'lineno': 5}]}, 'module_docstring': 'Implementaion of the loss-weighted variant of SoftAdapt.'}, 'normalized_slopes_variant.py': {'functions': {'__init__': {'code': '    def __init__(self, beta: int = 0.1, accuracy_order: int = None):\\n        \"\"\"SoftAdapt class initializer.\"\"\"\\n        super().__init__()\\n        self.beta = beta\\n        # Passing \"None\" as the order of accuracy sets the highest possible\\n        # accuracy in the finite difference approximation.\\n        self.accuracy_order = accuracy_order', 'lines': {'start': 25, 'end': 32}, 'docstring': 'SoftAdapt class initializer.'}, 'get_component_weights': {'code': '    def get_component_weights(self,\\n                               *loss_component_values: Tuple[torch.tensor],\\n                               verbose: bool = True):\\n        \"\"\"Class method for SoftAdapt weights.\\n\\n        Args:\\n            loss_component_values: A tuple consisting of the values of the each\\n              loss component that have been stored for the past \\'n\\' iterations\\n              or epochs (as described in the manuscript).\\n            verbose: A boolean indicating user preference for whether internal\\n              functions should print out information and warning about\\n              computations.\\n        Returns:\\n            The computed weights for each loss components. For example, if there\\n            were 5 loss components, say (l_1, l_2, l_3, l_4, l_5), then the\\n            return tensor will be the weights (alpha_1, alpha_2, alpha_3,\\n            alpha_4, alpha_5) in the order of the loss components.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        if len(loss_component_values) == 1:\\n            print(\"==> Warning: You have only passed on the values of one loss\"\\n                  \" component, which will result in trivial weighting.\")\\n\\n        rates_of_change = []\\n\\n        for loss_points in loss_component_values:\\n            # Compute the rates of change for each one of the loss components.\\n            rates_of_change.append(\\n                self._compute_rates_of_change(loss_points,\\n                                              self.accuracy_order,\\n                                              verbose=verbose))\\n\\n        rates_of_change = torch.tensor(rates_of_change)/torch.sum(\\n                                                torch.tensor(rates_of_change))\\n\\n        # Calculate the weight and return the values.\\n        return self._softmax(input_tensor=rates_of_change, beta=self.beta)', 'lines': {'start': 33, 'end': 73}, 'docstring': \"Class method for SoftAdapt weights.\\n\\nArgs:\\n    loss_component_values: A tuple consisting of the values of the each\\n      loss component that have been stored for the past 'n' iterations\\n      or epochs (as described in the manuscript).\\n    verbose: A boolean indicating user preference for whether internal\\n      functions should print out information and warning about\\n      computations.\\nReturns:\\n    The computed weights for each loss components. For example, if there\\n    were 5 loss components, say (l_1, l_2, l_3, l_4, l_5), then the\\n    return tensor will be the weights (alpha_1, alpha_2, alpha_3,\\n    alpha_4, alpha_5) in the order of the loss components.\\n\\nRaises:\\n    None.\"}}, 'classes': {'NormalizedSoftAdapt': {'code': 'class NormalizedSoftAdapt(SoftAdaptBase):\\n    \"\"\"The normalized-slopes variant class.\\n\\n    The normalized variant of SoftAdapt is described in section 3.1.3 of our\\n    manuscript (located at: https://arxiv.org/pdf/1912.12355.pdf).\\n\\n    Attributes:\\n        beta: A float that is the \\'beta\\' hyperparameter in our manuscript. If\\n          beta > 0, then softAdapt will pay more attention the worst performing\\n          loss component. If beta < 0, then SoftAdapt will assign higher weights\\n          to the better performing components. Beta==0 is the trivial case and\\n          all loss components will have coefficient 1.\\n\\n        accuracy_order: An integer indicating the accuracy order of the finite\\n          volume approximation of each loss component\\'s slope.\\n\\n    \"\"\"\\n\\n    def __init__(self, beta: int = 0.1, accuracy_order: int = None):\\n        \"\"\"SoftAdapt class initializer.\"\"\"\\n        super().__init__()\\n        self.beta = beta\\n        # Passing \"None\" as the order of accuracy sets the highest possible\\n        # accuracy in the finite difference approximation.\\n        self.accuracy_order = accuracy_order\\n\\n    def get_component_weights(self,\\n                               *loss_component_values: Tuple[torch.tensor],\\n                               verbose: bool = True):\\n        \"\"\"Class method for SoftAdapt weights.\\n\\n        Args:\\n            loss_component_values: A tuple consisting of the values of the each\\n              loss component that have been stored for the past \\'n\\' iterations\\n              or epochs (as described in the manuscript).\\n            verbose: A boolean indicating user preference for whether internal\\n              functions should print out information and warning about\\n              computations.\\n        Returns:\\n            The computed weights for each loss components. For example, if there\\n            were 5 loss components, say (l_1, l_2, l_3, l_4, l_5), then the\\n            return tensor will be the weights (alpha_1, alpha_2, alpha_3,\\n            alpha_4, alpha_5) in the order of the loss components.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        if len(loss_component_values) == 1:\\n            print(\"==> Warning: You have only passed on the values of one loss\"\\n                  \" component, which will result in trivial weighting.\")\\n\\n        rates_of_change = []\\n\\n        for loss_points in loss_component_values:\\n            # Compute the rates of change for each one of the loss components.\\n            rates_of_change.append(\\n                self._compute_rates_of_change(loss_points,\\n                                              self.accuracy_order,\\n                                              verbose=verbose))\\n\\n        rates_of_change = torch.tensor(rates_of_change)/torch.sum(\\n                                                torch.tensor(rates_of_change))\\n\\n        # Calculate the weight and return the values.\\n        return self._softmax(input_tensor=rates_of_change, beta=self.beta)', 'lines': {'start': 7, 'end': 73}, 'docstring': \"The normalized-slopes variant class.\\n\\nThe normalized variant of SoftAdapt is described in section 3.1.3 of our\\nmanuscript (located at: https://arxiv.org/pdf/1912.12355.pdf).\\n\\nAttributes:\\n    beta: A float that is the 'beta' hyperparameter in our manuscript. If\\n      beta > 0, then softAdapt will pay more attention the worst performing\\n      loss component. If beta < 0, then SoftAdapt will assign higher weights\\n      to the better performing components. Beta==0 is the trivial case and\\n      all loss components will have coefficient 1.\\n\\n    accuracy_order: An integer indicating the accuracy order of the finite\\n      volume approximation of each loss component's slope.\"}}, 'direct_imports': {'torch': {'alias': None, 'lineno': 3}}, 'from_imports': {'base._softadapt_base_class': [{'name': 'SoftAdaptBase', 'alias': None, 'lineno': 4}], 'typing': [{'name': 'Tuple', 'alias': None, 'lineno': 5}]}, 'module_docstring': 'Implementaion of the slope-normalized variant of SoftAdapt.'}, 'original_variant.py': {'functions': {'__init__': {'code': '    def __init__(self, beta: int = 0.1, accuracy_order: int = None):\\n        \"\"\"SoftAdapt class initializer.\"\"\"\\n        super().__init__()\\n        self.beta = beta\\n        # Passing \"None\" as the order of accuracy sets the highest possible\\n        # accuracy in the finite difference approximation.\\n        self.accuracy_order = accuracy_order', 'lines': {'start': 25, 'end': 32}, 'docstring': 'SoftAdapt class initializer.'}, 'get_component_weights': {'code': '    def get_component_weights(self,\\n                               *loss_component_values: Tuple[torch.tensor],\\n                               verbose: bool = True):\\n        \"\"\"Class method for SoftAdapt weights.\\n\\n        Args:\\n            loss_component_values: A tuple consisting of the values of the each\\n              loss component that have been stored for the past \\'n\\' iterations\\n              or epochs (as described in the manuscript).\\n            verbose: A boolean indicating user preference for whether internal\\n              functions should print out information and warning about\\n              computations.\\n        Returns:\\n            The computed weights for each loss components. For example, if there\\n            were 5 loss components, say (l_1, l_2, l_3, l_4, l_5), then the\\n            return tensor will be the weights (alpha_1, alpha_2, alpha_3,\\n            alpha_4, alpha_5) in the order of the loss components.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        if len(loss_component_values) == 1:\\n            print(\"==> Warning: You have only passed on the values of one loss\"\\n                  \" component, which will result in trivial weighting.\")\\n\\n        rates_of_change = []\\n\\n        for loss_points in loss_component_values:\\n            # Compute the rates of change for each one of the loss components.\\n            rates_of_change.append(\\n                self._compute_rates_of_change(loss_points,\\n                                              self.accuracy_order,\\n                                              verbose=verbose))\\n\\n        rates_of_change = torch.tensor(rates_of_change)\\n        # Calculate the weight and return the values.\\n        return self._softmax(input_tensor=rates_of_change, beta=self.beta)', 'lines': {'start': 33, 'end': 71}, 'docstring': \"Class method for SoftAdapt weights.\\n\\nArgs:\\n    loss_component_values: A tuple consisting of the values of the each\\n      loss component that have been stored for the past 'n' iterations\\n      or epochs (as described in the manuscript).\\n    verbose: A boolean indicating user preference for whether internal\\n      functions should print out information and warning about\\n      computations.\\nReturns:\\n    The computed weights for each loss components. For example, if there\\n    were 5 loss components, say (l_1, l_2, l_3, l_4, l_5), then the\\n    return tensor will be the weights (alpha_1, alpha_2, alpha_3,\\n    alpha_4, alpha_5) in the order of the loss components.\\n\\nRaises:\\n    None.\"}}, 'classes': {'SoftAdapt': {'code': 'class SoftAdapt(SoftAdaptBase):\\n    \"\"\"The original variant class.\\n\\n    The original variant of SoftAdapt is described in section 3.1.1 of our\\n    manuscript (located at: https://arxiv.org/pdf/1912.12355.pdf).\\n\\n    Attributes:\\n        beta: A float that is the \\'beta\\' hyperparameter in our manuscript. If\\n          beta > 0, then softAdapt will pay more attention the worst performing\\n          loss component. If beta < 0, then SoftAdapt will assign higher weights\\n          to the better performing components. Beta==0 is the trivial case and\\n          all loss components will have coefficient 1.\\n\\n        accuracy_order: An integer indicating the accuracy order of the finite\\n          volume approximation of each loss component\\'s slope.\\n\\n    \"\"\"\\n\\n    def __init__(self, beta: int = 0.1, accuracy_order: int = None):\\n        \"\"\"SoftAdapt class initializer.\"\"\"\\n        super().__init__()\\n        self.beta = beta\\n        # Passing \"None\" as the order of accuracy sets the highest possible\\n        # accuracy in the finite difference approximation.\\n        self.accuracy_order = accuracy_order\\n\\n    def get_component_weights(self,\\n                               *loss_component_values: Tuple[torch.tensor],\\n                               verbose: bool = True):\\n        \"\"\"Class method for SoftAdapt weights.\\n\\n        Args:\\n            loss_component_values: A tuple consisting of the values of the each\\n              loss component that have been stored for the past \\'n\\' iterations\\n              or epochs (as described in the manuscript).\\n            verbose: A boolean indicating user preference for whether internal\\n              functions should print out information and warning about\\n              computations.\\n        Returns:\\n            The computed weights for each loss components. For example, if there\\n            were 5 loss components, say (l_1, l_2, l_3, l_4, l_5), then the\\n            return tensor will be the weights (alpha_1, alpha_2, alpha_3,\\n            alpha_4, alpha_5) in the order of the loss components.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        if len(loss_component_values) == 1:\\n            print(\"==> Warning: You have only passed on the values of one loss\"\\n                  \" component, which will result in trivial weighting.\")\\n\\n        rates_of_change = []\\n\\n        for loss_points in loss_component_values:\\n            # Compute the rates of change for each one of the loss components.\\n            rates_of_change.append(\\n                self._compute_rates_of_change(loss_points,\\n                                              self.accuracy_order,\\n                                              verbose=verbose))\\n\\n        rates_of_change = torch.tensor(rates_of_change)\\n        # Calculate the weight and return the values.\\n        return self._softmax(input_tensor=rates_of_change, beta=self.beta)', 'lines': {'start': 7, 'end': 71}, 'docstring': \"The original variant class.\\n\\nThe original variant of SoftAdapt is described in section 3.1.1 of our\\nmanuscript (located at: https://arxiv.org/pdf/1912.12355.pdf).\\n\\nAttributes:\\n    beta: A float that is the 'beta' hyperparameter in our manuscript. If\\n      beta > 0, then softAdapt will pay more attention the worst performing\\n      loss component. If beta < 0, then SoftAdapt will assign higher weights\\n      to the better performing components. Beta==0 is the trivial case and\\n      all loss components will have coefficient 1.\\n\\n    accuracy_order: An integer indicating the accuracy order of the finite\\n      volume approximation of each loss component's slope.\"}}, 'direct_imports': {'torch': {'alias': None, 'lineno': 3}}, 'from_imports': {'base._softadapt_base_class': [{'name': 'SoftAdaptBase', 'alias': None, 'lineno': 4}], 'typing': [{'name': 'Tuple', 'alias': None, 'lineno': 5}]}, 'module_docstring': 'Implementaion of the original variant of SoftAdapt.'}, '__init__.py': {'functions': {}, 'classes': {}, 'direct_imports': {}, 'from_imports': {'loss_weighted_variant': [{'name': 'LossWeightedSoftAdapt', 'alias': None, 'lineno': 2}], 'normalized_slopes_variant': [{'name': 'NormalizedSoftAdapt', 'alias': None, 'lineno': 3}], 'original_variant': [{'name': 'SoftAdapt', 'alias': None, 'lineno': 4}]}, 'module_docstring': 'Third level module import for SoftAdapt variants.'}}, 'base': {'_softadapt_base_class.py': {'functions': {'__init__': {'code': '    def __init__(self):\\n        \"\"\"Initializer of the base method.\"\"\"\\n        self.epsilon = _EPSILON', 'lines': {'start': 16, 'end': 19}, 'docstring': 'Initializer of the base method.'}, '_softmax': {'code': '    def _softmax(self,\\n                 input_tensor: torch.tensor,\\n                 beta: float = 1,\\n                 numerator_weights: torch.tensor = None,\\n                 shift_by_max_value: bool = True):\\n        \"\"\"Implementation of SoftAdapt\\'s modified softmax function.\\n\\n        Args:\\n            input_tensor: A tensor of floats which will be used for computing\\n              the (modified) softmax function.\\n            beta: A float which is the scaling factor (as described in the\\n              manuscript).\\n            numerator_weights: A tensor of weights which are the actual value of\\n              of the loss components. This option is used for the\\n              \"loss-weighted\" variant of SoftAdapt.\\n            shift_by_max_value: A boolean indicating whether we want the values\\n              in the input tensor to be shifted by the maximum value.\\n\\n        Returns:\\n            A tensor of floats that are the softmax results.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        if shift_by_max_value:\\n            exp_of_input = torch.exp(beta * (input_tensor - input_tensor.max()))\\n        else:\\n            exp_of_input = torch.exp(beta * input_tensor)\\n\\n        # This option will be used for the \"loss-weighted\" variant of SoftAdapt.\\n        if numerator_weights is not None:\\n            exp_of_input = torch.multiply(numerator_weights, exp_of_input)\\n\\n        return exp_of_input / (torch.sum(exp_of_input) + self.epsilon)', 'lines': {'start': 20, 'end': 55}, 'docstring': 'Implementation of SoftAdapt\\'s modified softmax function.\\n\\nArgs:\\n    input_tensor: A tensor of floats which will be used for computing\\n      the (modified) softmax function.\\n    beta: A float which is the scaling factor (as described in the\\n      manuscript).\\n    numerator_weights: A tensor of weights which are the actual value of\\n      of the loss components. This option is used for the\\n      \"loss-weighted\" variant of SoftAdapt.\\n    shift_by_max_value: A boolean indicating whether we want the values\\n      in the input tensor to be shifted by the maximum value.\\n\\nReturns:\\n    A tensor of floats that are the softmax results.\\n\\nRaises:\\n    None.'}, '_compute_rates_of_change': {'code': '    def _compute_rates_of_change(self,\\n                                 input_tensor:torch.tensor,\\n                                 order: int = 5,\\n                                 verbose: bool = True):\\n        \"\"\"Base class method for computing loss functions rate of change.\\n\\n        Args:\\n            input_tensor: A tensor of floats containing loss evaluations at the\\n              previous \\'n\\' points (as many points as the order) of the finite\\n              difference method.\\n            order: An integer indicating the order of the finite difference\\n              method we want to use. The function will use the length of the\\n              \\'input_array\\' array if no values is provided.\\n            verbose: Whether we want the function to print out information about\\n              computations or not.\\n\\n        Returns:\\n            The approximated derivative as a float value.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        return _get_finite_difference(input_array = input_tensor.numpy(),\\n                                      order = order,\\n                                      verbose = verbose)', 'lines': {'start': 57, 'end': 83}, 'docstring': \"Base class method for computing loss functions rate of change.\\n\\nArgs:\\n    input_tensor: A tensor of floats containing loss evaluations at the\\n      previous 'n' points (as many points as the order) of the finite\\n      difference method.\\n    order: An integer indicating the order of the finite difference\\n      method we want to use. The function will use the length of the\\n      'input_array' array if no values is provided.\\n    verbose: Whether we want the function to print out information about\\n      computations or not.\\n\\nReturns:\\n    The approximated derivative as a float value.\\n\\nRaises:\\n    None.\"}}, 'classes': {'SoftAdaptBase': {'code': 'class SoftAdaptBase():\\n    \"\"\"Base model for any of the SoftAdapt variants.\\n\\n    Attributes:\\n        epsilon: A float which is added to the denominator of a division for\\n          numerical stability.\\n\\n    \"\"\"\\n\\n    def __init__(self):\\n        \"\"\"Initializer of the base method.\"\"\"\\n        self.epsilon = _EPSILON\\n\\n    def _softmax(self,\\n                 input_tensor: torch.tensor,\\n                 beta: float = 1,\\n                 numerator_weights: torch.tensor = None,\\n                 shift_by_max_value: bool = True):\\n        \"\"\"Implementation of SoftAdapt\\'s modified softmax function.\\n\\n        Args:\\n            input_tensor: A tensor of floats which will be used for computing\\n              the (modified) softmax function.\\n            beta: A float which is the scaling factor (as described in the\\n              manuscript).\\n            numerator_weights: A tensor of weights which are the actual value of\\n              of the loss components. This option is used for the\\n              \"loss-weighted\" variant of SoftAdapt.\\n            shift_by_max_value: A boolean indicating whether we want the values\\n              in the input tensor to be shifted by the maximum value.\\n\\n        Returns:\\n            A tensor of floats that are the softmax results.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        if shift_by_max_value:\\n            exp_of_input = torch.exp(beta * (input_tensor - input_tensor.max()))\\n        else:\\n            exp_of_input = torch.exp(beta * input_tensor)\\n\\n        # This option will be used for the \"loss-weighted\" variant of SoftAdapt.\\n        if numerator_weights is not None:\\n            exp_of_input = torch.multiply(numerator_weights, exp_of_input)\\n\\n        return exp_of_input / (torch.sum(exp_of_input) + self.epsilon)\\n\\n\\n    def _compute_rates_of_change(self,\\n                                 input_tensor:torch.tensor,\\n                                 order: int = 5,\\n                                 verbose: bool = True):\\n        \"\"\"Base class method for computing loss functions rate of change.\\n\\n        Args:\\n            input_tensor: A tensor of floats containing loss evaluations at the\\n              previous \\'n\\' points (as many points as the order) of the finite\\n              difference method.\\n            order: An integer indicating the order of the finite difference\\n              method we want to use. The function will use the length of the\\n              \\'input_array\\' array if no values is provided.\\n            verbose: Whether we want the function to print out information about\\n              computations or not.\\n\\n        Returns:\\n            The approximated derivative as a float value.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        return _get_finite_difference(input_array = input_tensor.numpy(),\\n                                      order = order,\\n                                      verbose = verbose)', 'lines': {'start': 7, 'end': 83}, 'docstring': 'Base model for any of the SoftAdapt variants.\\n\\nAttributes:\\n    epsilon: A float which is added to the denominator of a division for\\n      numerical stability.'}}, 'direct_imports': {'torch': {'alias': None, 'lineno': 3}}, 'from_imports': {'constants._stability_constants': [{'name': '_EPSILON', 'alias': None, 'lineno': 4}], 'utilities._finite_difference': [{'name': '_get_finite_difference', 'alias': None, 'lineno': 5}]}, 'module_docstring': 'Implementaion of the base class for SoftAdapt.'}, '__init__.py': {'functions': {}, 'classes': {}, 'direct_imports': {}, 'from_imports': {}, 'module_docstring': 'Third level import for base class of SoftAdapt.'}}, 'constants': {'_finite_difference_constants.py': {'functions': {}, 'classes': {}, 'direct_imports': {'numpy': {'alias': None, 'lineno': 2}}, 'from_imports': {}, 'module_docstring': 'Definition of constants for odd finite difference (up to 5)'}, '_stability_constants.py': {'functions': {}, 'classes': {}, 'direct_imports': {}, 'from_imports': {}, 'module_docstring': 'Definition of constants for numerical stability purposes.'}, '__init__.py': {'functions': {}, 'classes': {}, 'direct_imports': {}, 'from_imports': {}, 'module_docstring': 'Third level module import for constants.'}}, 'utilities': {'_finite_difference.py': {'functions': {'_get_finite_difference': {'code': 'def _get_finite_difference(input_array: numpy.array,\\n                           order: int = None,\\n                           verbose: bool = True):\\n    \"\"\"Internal utility method for estimating rate of change.\\n\\n    This function aims to approximate the rate of change for a loss function,\\n    which is used for the \\'LossWeighted\\' and \\'Normalized\\' variants of SoftAdapt.\\n\\n    For even accuracy orders, we take advantage of the `findiff` package\\n    (https://findiff.readthedocs.io/en/latest/source/examples-basic.html).\\n    Accuracy orders of 1 (trivial), 3, and 5 are retrieved from an internal\\n    constants file. Due to the underlying mathematics of computing the\\n    coefficients, all accuracy orders higher than 5 must be an even number.\\n\\n    Args:\\n        input_array: An array of floats containing loss evaluations at the\\n          previous \\'n\\' points (as many points as the order) of the finite\\n          difference method.\\n        order: An integer indicating the order of the finite difference method\\n          we want to use. The function will use the length of the \\'input_array\\'\\n          array if no values is provided.\\n        verbose: Whether we want the function to print out information about\\n          computations or not.\\n\\n    Returns:\\n        A float which is the approximated rate of change between the loss\\n        points.\\n\\n    Raises:\\n        ValueError: If the number of points in the `input_array` array is\\n          smaller than the order of accuracy we desire.\\n        Value Error: If the order of accuracy is higher than 5 and it is not an\\n          even number.\\n    \"\"\"\\n    # First, we want to check the order and the number of loss points we are\\n    # given\\n    if order is None:\\n        order = len(input_array) - 1\\n        if verbose:\\n            print(f\"==> Interpreting finite difference order as {order} since\"\\n                  \"no explicit order was specified.\")\\n    else:\\n        if order > len(input_array):\\n            raise ValueError(\"The order of finite difference computations can\"\\n                             \"not be larger than the number of loss points. \"\\n                             \"Please check the order argument or wait until \"\\n                             \"enough points have been stored before calling the\"\\n                             \" method.\")\\n        elif order + 1 < len(input_array):\\n            print(f\"==> There are more points than \\'order\\' + 1 ({order + 1}) \"\\n                  f\"points (array contains {len(input_array)} values). Function\"\\n                  f\"will use the last {order} elements of loss points for \"\\n                  \"computations.\")\\n            input_array = input_array[(-1*order - 1):]\\n\\n    order_is_even = order % 2 == 0\\n    # Next, we want to retrieve the correct coefficients based on the order\\n    if order > 5 and not order_is_even:\\n        raise ValueError(\"Accuracy orders larger than 5 must be even. Please \"\\n                         \"check the arguments passed to the function.\")\\n\\n    if order_is_even:\\n        constants = coefficients(deriv=1, acc=order)[\"forward\"][\"coefficients\"]\\n\\n    else:\\n        if order == 1:\\n            constants = _FIRST_ORDER_COEFFICIENTS\\n        elif order == 3:\\n            constants = _THIRD_ORDER_COEFFICIENTS\\n        else:\\n            constants = _FIFTH_ORDER_COEFFICIENTS\\n\\n    pointwise_multiplication = [\\n        input_array[i] * constants[i] for i in range(len(constants))\\n    ]\\n    return numpy.sum(pointwise_multiplication)', 'lines': {'start': 7, 'end': 83}, 'docstring': \"Internal utility method for estimating rate of change.\\n\\nThis function aims to approximate the rate of change for a loss function,\\nwhich is used for the 'LossWeighted' and 'Normalized' variants of SoftAdapt.\\n\\nFor even accuracy orders, we take advantage of the `findiff` package\\n(https://findiff.readthedocs.io/en/latest/source/examples-basic.html).\\nAccuracy orders of 1 (trivial), 3, and 5 are retrieved from an internal\\nconstants file. Due to the underlying mathematics of computing the\\ncoefficients, all accuracy orders higher than 5 must be an even number.\\n\\nArgs:\\n    input_array: An array of floats containing loss evaluations at the\\n      previous 'n' points (as many points as the order) of the finite\\n      difference method.\\n    order: An integer indicating the order of the finite difference method\\n      we want to use. The function will use the length of the 'input_array'\\n      array if no values is provided.\\n    verbose: Whether we want the function to print out information about\\n      computations or not.\\n\\nReturns:\\n    A float which is the approximated rate of change between the loss\\n    points.\\n\\nRaises:\\n    ValueError: If the number of points in the `input_array` array is\\n      smaller than the order of accuracy we desire.\\n    Value Error: If the order of accuracy is higher than 5 and it is not an\\n      even number.\"}}, 'classes': {}, 'direct_imports': {'numpy': {'alias': None, 'lineno': 2}}, 'from_imports': {'findiff': [{'name': 'coefficients', 'alias': None, 'lineno': 3}], 'constants._finite_difference_constants': [{'name': '_FIRST_ORDER_COEFFICIENTS', 'alias': None, 'lineno': 4}, {'name': '_THIRD_ORDER_COEFFICIENTS', 'alias': None, 'lineno': 4}, {'name': '_FIFTH_ORDER_COEFFICIENTS', 'alias': None, 'lineno': 4}]}, 'module_docstring': 'Internal implementation of '}, '__init__.py': {'functions': {}, 'classes': {}, 'direct_imports': {}, 'from_imports': {}, 'module_docstring': 'Third level import for SoftAdapt utilities.'}}, '__init__.py': {'functions': {}, 'classes': {}, 'direct_imports': {'importlib.metadata': {'alias': 'importlib_metadata', 'lineno': 8}, 'importlib_metadata': {'alias': None, 'lineno': 10}}, 'from_imports': {'algorithms': [{'name': '*', 'alias': None, 'lineno': 2}], 'constants': [{'name': '*', 'alias': None, 'lineno': 3}], 'utilities': [{'name': '*', 'alias': None, 'lineno': 4}]}, 'module_docstring': 'Second level module import for SoftAdapt.'}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "github_url = \"https://github.com/dr-aheydari/SoftAdapt\"\n",
    "output_dir_path = r\"C:\\Users\\agianolini\\OneDrive - ANDES WEALTH MANAGEMENT SA\\Desktop\\research-assistant-main\"\n",
    "model_path = r\"C:\\Users\\agianolini\\OneDrive - ANDES WEALTH MANAGEMENT SA\\Desktop\\research-assistant-main\\model (7).bin\"\n",
    "nl_query = 'a for loop'\n",
    "\n",
    "\n",
    "\n",
    "temp_dir = r\"C:\\Users\\agianolini\\OneDrive - ANDES WEALTH MANAGEMENT SA\\Desktop\\research-assistant-main\\output\" # Temporary directory to clone the repo\n",
    "clone_github_repo(github_url, temp_dir)\n",
    "repo_name = get_last_segment(github_url)\n",
    "\n",
    "# Get the project structure\n",
    "project_structure = get_project_structure(temp_dir )\n",
    "\n",
    "# Save the project structure to a JSON file\n",
    "os.makedirs(output_dir_path, exist_ok=True)\n",
    "project_name = os.path.basename(github_url)\n",
    "output_path = os.path.join(output_dir_path, f'{project_name}.json')\n",
    "\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(project_structure, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "data = {}\n",
    "# Leer y analizar el archivo JSON\n",
    "try:\n",
    "    with open(output_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer el archivo JSON: {e}\")\n",
    "\n",
    "# Proceso de extracción de fragmentos de código\n",
    "code_snippets = []\n",
    "code_snippets = extract_code_snippets(data)\n",
    "# try:\n",
    "# # Iterar sobre todas las claves principales del diccionario JSON\n",
    "#     for main_key in data.keys():\n",
    "#         for filename, content in data[main_key].items():\n",
    "#             if 'functions' in content:\n",
    "#                 for function_name, function_content in content['functions'].items():\n",
    "#                     if 'code' in function_content:\n",
    "#                         code_snippets.append(function_content['code'])\n",
    "# except Exception as e:\n",
    "#     code_snippets = f\"Error al procesar el JSON: {e}\"\n",
    "\n",
    "# searcher = CodeSearcher(model_path, code_snippets)\n",
    "# k=3\n",
    "# t = searcher.get_similarity_search(nl_query, k)\n",
    "\n",
    "# print(\"Top K similar items: \\n\")\n",
    "# print(t)\n",
    "\n",
    "# print(data)\n",
    "print(code_snippets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    def __init__(self, beta: int = 0.1, accuracy_order: int = None):\\n        \"\"\"SoftAdapt class initializer.\"\"\"\\n        super().__init__()\\n        self.beta = beta\\n        # Passing \"None\" as the order of accuracy sets the highest possible\\n        # accura',\n",
       " 'cy in the finite difference approximation.\\n        self.accuracy_order = accuracy_order',\n",
       " '    def get_component_weights(self,\\n                               *loss_component_values: Tuple[torch.tensor],\\n                               verbose: bool = True):\\n        \"\"\"Class method for SoftAdapt weights.\\n\\n        Args:\\n            loss_component_v',\n",
       " \"alues: A tuple consisting of the values of the each\\n              loss component that have been stored for the past 'n' iterations\\n              or epochs (as described in the manuscript).\\n            verbose: A boolean indicating user preference for wheth\",\n",
       " 'er internal\\n              functions should print out information and warning about\\n              computations.\\n        Returns:\\n            The computed weights for each loss components. For example, if there\\n            were 5 loss components, say (l_1, l',\n",
       " '_2, l_3, l_4, l_5), then the\\n            return tensor will be the weights (alpha_1, alpha_2, alpha_3,\\n            alpha_4, alpha_5) in the order of the loss components.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        if len(loss_component_values) ',\n",
       " '== 1:\\n            print(\"==> Warning: You have only passed on the values of one loss\"\\n                  \" component, which will result in trivial weighting.\")\\n\\n        rates_of_change = []\\n        average_loss_values = []\\n\\n        for loss_points in loss_c',\n",
       " 'omponent_values:\\n            # Compute the rates of change for each one of the loss components.\\n            rates_of_change.append(\\n                self._compute_rates_of_change(loss_points,\\n                                              self.accuracy_order',\n",
       " ',\\n                                              verbose=verbose))\\n            average_loss_values.append(torch.mean(loss_points.float()))\\n\\n        rates_of_change = torch.tensor(rates_of_change)\\n        average_loss_values = torch.tensor(average_loss_value',\n",
       " 's)\\n        # Calculate the weight and return the values.\\n        return self._softmax(input_tensor=rates_of_change,\\n                             beta=self.beta,\\n                             numerator_weights = average_loss_values,\\n                         ',\n",
       " '    )',\n",
       " 'class LossWeightedSoftAdapt(SoftAdaptBase):\\n    \"\"\"Class implementation of the loss-weighted SoftAdapt variant.\\n\\n    The loss-weighted variant of SoftAdapt is described in section 3.1.1 of our\\n    manuscript (located at: https://arxiv.org/pdf/1912.12355.pd',\n",
       " \"f).\\n\\n    Attributes:\\n        beta: A float that is the 'beta' hyperparameter in our manuscript. If\\n          beta > 0, then softAdapt will pay more attention the worst performing\\n          loss component. If beta < 0, then SoftAdapt will assign higher weig\",\n",
       " 'hts\\n          to the better performing components. Beta==0 is the trivial case and\\n          all loss components will have coefficient 1.\\n\\n        accuracy_order: An integer indicating the accuracy order of the finite\\n          volume approximation of each',\n",
       " ' loss component\\'s slope.\\n    \"\"\"\\n\\n    def __init__(self, beta: int = 0.1, accuracy_order: int = None):\\n        \"\"\"SoftAdapt class initializer.\"\"\"\\n        super().__init__()\\n        self.beta = beta\\n        # Passing \"None\" as the order of accuracy sets the',\n",
       " ' highest possible\\n        # accuracy in the finite difference approximation.\\n        self.accuracy_order = accuracy_order\\n\\n    def get_component_weights(self,\\n                               *loss_component_values: Tuple[torch.tensor],\\n                     ',\n",
       " '          verbose: bool = True):\\n        \"\"\"Class method for SoftAdapt weights.\\n\\n        Args:\\n            loss_component_values: A tuple consisting of the values of the each\\n              loss component that have been stored for the past \\'n\\' iterations\\n  ',\n",
       " '            or epochs (as described in the manuscript).\\n            verbose: A boolean indicating user preference for whether internal\\n              functions should print out information and warning about\\n              computations.\\n        Returns:\\n     ',\n",
       " '       The computed weights for each loss components. For example, if there\\n            were 5 loss components, say (l_1, l_2, l_3, l_4, l_5), then the\\n            return tensor will be the weights (alpha_1, alpha_2, alpha_3,\\n            alpha_4, alpha_5) ',\n",
       " 'in the order of the loss components.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        if len(loss_component_values) == 1:\\n            print(\"==> Warning: You have only passed on the values of one loss\"\\n                  \" component, which will resul',\n",
       " 't in trivial weighting.\")\\n\\n        rates_of_change = []\\n        average_loss_values = []\\n\\n        for loss_points in loss_component_values:\\n            # Compute the rates of change for each one of the loss components.\\n            rates_of_change.append(\\n ',\n",
       " '               self._compute_rates_of_change(loss_points,\\n                                              self.accuracy_order,\\n                                              verbose=verbose))\\n            average_loss_values.append(torch.mean(loss_points.float',\n",
       " '()))\\n\\n        rates_of_change = torch.tensor(rates_of_change)\\n        average_loss_values = torch.tensor(average_loss_values)\\n        # Calculate the weight and return the values.\\n        return self._softmax(input_tensor=rates_of_change,\\n                 ',\n",
       " '            beta=self.beta,\\n                             numerator_weights = average_loss_values,\\n                             )',\n",
       " '    def __init__(self, beta: int = 0.1, accuracy_order: int = None):\\n        \"\"\"SoftAdapt class initializer.\"\"\"\\n        super().__init__()\\n        self.beta = beta\\n        # Passing \"None\" as the order of accuracy sets the highest possible\\n        # accura',\n",
       " 'cy in the finite difference approximation.\\n        self.accuracy_order = accuracy_order',\n",
       " '    def get_component_weights(self,\\n                               *loss_component_values: Tuple[torch.tensor],\\n                               verbose: bool = True):\\n        \"\"\"Class method for SoftAdapt weights.\\n\\n        Args:\\n            loss_component_v',\n",
       " \"alues: A tuple consisting of the values of the each\\n              loss component that have been stored for the past 'n' iterations\\n              or epochs (as described in the manuscript).\\n            verbose: A boolean indicating user preference for wheth\",\n",
       " 'er internal\\n              functions should print out information and warning about\\n              computations.\\n        Returns:\\n            The computed weights for each loss components. For example, if there\\n            were 5 loss components, say (l_1, l',\n",
       " '_2, l_3, l_4, l_5), then the\\n            return tensor will be the weights (alpha_1, alpha_2, alpha_3,\\n            alpha_4, alpha_5) in the order of the loss components.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        if len(loss_component_values) ',\n",
       " '== 1:\\n            print(\"==> Warning: You have only passed on the values of one loss\"\\n                  \" component, which will result in trivial weighting.\")\\n\\n        rates_of_change = []\\n\\n        for loss_points in loss_component_values:\\n            # Co',\n",
       " 'mpute the rates of change for each one of the loss components.\\n            rates_of_change.append(\\n                self._compute_rates_of_change(loss_points,\\n                                              self.accuracy_order,\\n                               ',\n",
       " '               verbose=verbose))\\n\\n        rates_of_change = torch.tensor(rates_of_change)/torch.sum(\\n                                                torch.tensor(rates_of_change))\\n\\n        # Calculate the weight and return the values.\\n        return self._',\n",
       " 'softmax(input_tensor=rates_of_change, beta=self.beta)',\n",
       " 'class NormalizedSoftAdapt(SoftAdaptBase):\\n    \"\"\"The normalized-slopes variant class.\\n\\n    The normalized variant of SoftAdapt is described in section 3.1.3 of our\\n    manuscript (located at: https://arxiv.org/pdf/1912.12355.pdf).\\n\\n    Attributes:\\n        ',\n",
       " \"beta: A float that is the 'beta' hyperparameter in our manuscript. If\\n          beta > 0, then softAdapt will pay more attention the worst performing\\n          loss component. If beta < 0, then SoftAdapt will assign higher weights\\n          to the better p\",\n",
       " \"erforming components. Beta==0 is the trivial case and\\n          all loss components will have coefficient 1.\\n\\n        accuracy_order: An integer indicating the accuracy order of the finite\\n          volume approximation of each loss component's slope.\\n\\n   \",\n",
       " ' \"\"\"\\n\\n    def __init__(self, beta: int = 0.1, accuracy_order: int = None):\\n        \"\"\"SoftAdapt class initializer.\"\"\"\\n        super().__init__()\\n        self.beta = beta\\n        # Passing \"None\" as the order of accuracy sets the highest possible\\n        # ',\n",
       " 'accuracy in the finite difference approximation.\\n        self.accuracy_order = accuracy_order\\n\\n    def get_component_weights(self,\\n                               *loss_component_values: Tuple[torch.tensor],\\n                               verbose: bool = Tr',\n",
       " 'ue):\\n        \"\"\"Class method for SoftAdapt weights.\\n\\n        Args:\\n            loss_component_values: A tuple consisting of the values of the each\\n              loss component that have been stored for the past \\'n\\' iterations\\n              or epochs (as de',\n",
       " 'scribed in the manuscript).\\n            verbose: A boolean indicating user preference for whether internal\\n              functions should print out information and warning about\\n              computations.\\n        Returns:\\n            The computed weights ',\n",
       " 'for each loss components. For example, if there\\n            were 5 loss components, say (l_1, l_2, l_3, l_4, l_5), then the\\n            return tensor will be the weights (alpha_1, alpha_2, alpha_3,\\n            alpha_4, alpha_5) in the order of the loss com',\n",
       " 'ponents.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        if len(loss_component_values) == 1:\\n            print(\"==> Warning: You have only passed on the values of one loss\"\\n                  \" component, which will result in trivial weighting.\")\\n\\n ',\n",
       " '       rates_of_change = []\\n\\n        for loss_points in loss_component_values:\\n            # Compute the rates of change for each one of the loss components.\\n            rates_of_change.append(\\n                self._compute_rates_of_change(loss_points,\\n   ',\n",
       " '                                           self.accuracy_order,\\n                                              verbose=verbose))\\n\\n        rates_of_change = torch.tensor(rates_of_change)/torch.sum(\\n                                                torch.tensor',\n",
       " '(rates_of_change))\\n\\n        # Calculate the weight and return the values.\\n        return self._softmax(input_tensor=rates_of_change, beta=self.beta)',\n",
       " '    def __init__(self, beta: int = 0.1, accuracy_order: int = None):\\n        \"\"\"SoftAdapt class initializer.\"\"\"\\n        super().__init__()\\n        self.beta = beta\\n        # Passing \"None\" as the order of accuracy sets the highest possible\\n        # accura',\n",
       " 'cy in the finite difference approximation.\\n        self.accuracy_order = accuracy_order',\n",
       " '    def get_component_weights(self,\\n                               *loss_component_values: Tuple[torch.tensor],\\n                               verbose: bool = True):\\n        \"\"\"Class method for SoftAdapt weights.\\n\\n        Args:\\n            loss_component_v',\n",
       " \"alues: A tuple consisting of the values of the each\\n              loss component that have been stored for the past 'n' iterations\\n              or epochs (as described in the manuscript).\\n            verbose: A boolean indicating user preference for wheth\",\n",
       " 'er internal\\n              functions should print out information and warning about\\n              computations.\\n        Returns:\\n            The computed weights for each loss components. For example, if there\\n            were 5 loss components, say (l_1, l',\n",
       " '_2, l_3, l_4, l_5), then the\\n            return tensor will be the weights (alpha_1, alpha_2, alpha_3,\\n            alpha_4, alpha_5) in the order of the loss components.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        if len(loss_component_values) ',\n",
       " '== 1:\\n            print(\"==> Warning: You have only passed on the values of one loss\"\\n                  \" component, which will result in trivial weighting.\")\\n\\n        rates_of_change = []\\n\\n        for loss_points in loss_component_values:\\n            # Co',\n",
       " 'mpute the rates of change for each one of the loss components.\\n            rates_of_change.append(\\n                self._compute_rates_of_change(loss_points,\\n                                              self.accuracy_order,\\n                               ',\n",
       " '               verbose=verbose))\\n\\n        rates_of_change = torch.tensor(rates_of_change)\\n        # Calculate the weight and return the values.\\n        return self._softmax(input_tensor=rates_of_change, beta=self.beta)',\n",
       " 'class SoftAdapt(SoftAdaptBase):\\n    \"\"\"The original variant class.\\n\\n    The original variant of SoftAdapt is described in section 3.1.1 of our\\n    manuscript (located at: https://arxiv.org/pdf/1912.12355.pdf).\\n\\n    Attributes:\\n        beta: A float that is',\n",
       " \" the 'beta' hyperparameter in our manuscript. If\\n          beta > 0, then softAdapt will pay more attention the worst performing\\n          loss component. If beta < 0, then SoftAdapt will assign higher weights\\n          to the better performing components.\",\n",
       " ' Beta==0 is the trivial case and\\n          all loss components will have coefficient 1.\\n\\n        accuracy_order: An integer indicating the accuracy order of the finite\\n          volume approximation of each loss component\\'s slope.\\n\\n    \"\"\"\\n\\n    def __init_',\n",
       " '_(self, beta: int = 0.1, accuracy_order: int = None):\\n        \"\"\"SoftAdapt class initializer.\"\"\"\\n        super().__init__()\\n        self.beta = beta\\n        # Passing \"None\" as the order of accuracy sets the highest possible\\n        # accuracy in the finit',\n",
       " 'e difference approximation.\\n        self.accuracy_order = accuracy_order\\n\\n    def get_component_weights(self,\\n                               *loss_component_values: Tuple[torch.tensor],\\n                               verbose: bool = True):\\n        \"\"\"Class',\n",
       " \" method for SoftAdapt weights.\\n\\n        Args:\\n            loss_component_values: A tuple consisting of the values of the each\\n              loss component that have been stored for the past 'n' iterations\\n              or epochs (as described in the manusc\",\n",
       " 'ript).\\n            verbose: A boolean indicating user preference for whether internal\\n              functions should print out information and warning about\\n              computations.\\n        Returns:\\n            The computed weights for each loss compone',\n",
       " 'nts. For example, if there\\n            were 5 loss components, say (l_1, l_2, l_3, l_4, l_5), then the\\n            return tensor will be the weights (alpha_1, alpha_2, alpha_3,\\n            alpha_4, alpha_5) in the order of the loss components.\\n\\n        Rai',\n",
       " 'ses:\\n            None.\\n\\n        \"\"\"\\n        if len(loss_component_values) == 1:\\n            print(\"==> Warning: You have only passed on the values of one loss\"\\n                  \" component, which will result in trivial weighting.\")\\n\\n        rates_of_chang',\n",
       " 'e = []\\n\\n        for loss_points in loss_component_values:\\n            # Compute the rates of change for each one of the loss components.\\n            rates_of_change.append(\\n                self._compute_rates_of_change(loss_points,\\n                        ',\n",
       " '                      self.accuracy_order,\\n                                              verbose=verbose))\\n\\n        rates_of_change = torch.tensor(rates_of_change)\\n        # Calculate the weight and return the values.\\n        return self._softmax(input_ten',\n",
       " 'sor=rates_of_change, beta=self.beta)',\n",
       " '    def __init__(self):\\n        \"\"\"Initializer of the base method.\"\"\"\\n        self.epsilon = _EPSILON',\n",
       " '    def _softmax(self,\\n                 input_tensor: torch.tensor,\\n                 beta: float = 1,\\n                 numerator_weights: torch.tensor = None,\\n                 shift_by_max_value: bool = True):\\n        \"\"\"Implementation of SoftAdapt\\'s modif',\n",
       " 'ied softmax function.\\n\\n        Args:\\n            input_tensor: A tensor of floats which will be used for computing\\n              the (modified) softmax function.\\n            beta: A float which is the scaling factor (as described in the\\n              manus',\n",
       " 'cript).\\n            numerator_weights: A tensor of weights which are the actual value of\\n              of the loss components. This option is used for the\\n              \"loss-weighted\" variant of SoftAdapt.\\n            shift_by_max_value: A boolean indicat',\n",
       " 'ing whether we want the values\\n              in the input tensor to be shifted by the maximum value.\\n\\n        Returns:\\n            A tensor of floats that are the softmax results.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        if shift_by_max_valu',\n",
       " 'e:\\n            exp_of_input = torch.exp(beta * (input_tensor - input_tensor.max()))\\n        else:\\n            exp_of_input = torch.exp(beta * input_tensor)\\n\\n        # This option will be used for the \"loss-weighted\" variant of SoftAdapt.\\n        if numerat',\n",
       " 'or_weights is not None:\\n            exp_of_input = torch.multiply(numerator_weights, exp_of_input)\\n\\n        return exp_of_input / (torch.sum(exp_of_input) + self.epsilon)',\n",
       " '    def _compute_rates_of_change(self,\\n                                 input_tensor:torch.tensor,\\n                                 order: int = 5,\\n                                 verbose: bool = True):\\n        \"\"\"Base class method for computing loss func',\n",
       " \"tions rate of change.\\n\\n        Args:\\n            input_tensor: A tensor of floats containing loss evaluations at the\\n              previous 'n' points (as many points as the order) of the finite\\n              difference method.\\n            order: An intege\",\n",
       " \"r indicating the order of the finite difference\\n              method we want to use. The function will use the length of the\\n              'input_array' array if no values is provided.\\n            verbose: Whether we want the function to print out informat\",\n",
       " 'ion about\\n              computations or not.\\n\\n        Returns:\\n            The approximated derivative as a float value.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        return _get_finite_difference(input_array = input_tensor.numpy(),\\n             ',\n",
       " '                         order = order,\\n                                      verbose = verbose)',\n",
       " 'class SoftAdaptBase():\\n    \"\"\"Base model for any of the SoftAdapt variants.\\n\\n    Attributes:\\n        epsilon: A float which is added to the denominator of a division for\\n          numerical stability.\\n\\n    \"\"\"\\n\\n    def __init__(self):\\n        \"\"\"Initialize',\n",
       " 'r of the base method.\"\"\"\\n        self.epsilon = _EPSILON\\n\\n    def _softmax(self,\\n                 input_tensor: torch.tensor,\\n                 beta: float = 1,\\n                 numerator_weights: torch.tensor = None,\\n                 shift_by_max_value: bo',\n",
       " 'ol = True):\\n        \"\"\"Implementation of SoftAdapt\\'s modified softmax function.\\n\\n        Args:\\n            input_tensor: A tensor of floats which will be used for computing\\n              the (modified) softmax function.\\n            beta: A float which is t',\n",
       " 'he scaling factor (as described in the\\n              manuscript).\\n            numerator_weights: A tensor of weights which are the actual value of\\n              of the loss components. This option is used for the\\n              \"loss-weighted\" variant of So',\n",
       " 'ftAdapt.\\n            shift_by_max_value: A boolean indicating whether we want the values\\n              in the input tensor to be shifted by the maximum value.\\n\\n        Returns:\\n            A tensor of floats that are the softmax results.\\n\\n        Raises:\\n ',\n",
       " '           None.\\n\\n        \"\"\"\\n        if shift_by_max_value:\\n            exp_of_input = torch.exp(beta * (input_tensor - input_tensor.max()))\\n        else:\\n            exp_of_input = torch.exp(beta * input_tensor)\\n\\n        # This option will be used for th',\n",
       " 'e \"loss-weighted\" variant of SoftAdapt.\\n        if numerator_weights is not None:\\n            exp_of_input = torch.multiply(numerator_weights, exp_of_input)\\n\\n        return exp_of_input / (torch.sum(exp_of_input) + self.epsilon)\\n\\n\\n    def _compute_rates_of',\n",
       " '_change(self,\\n                                 input_tensor:torch.tensor,\\n                                 order: int = 5,\\n                                 verbose: bool = True):\\n        \"\"\"Base class method for computing loss functions rate of change.\\n\\n  ',\n",
       " \"      Args:\\n            input_tensor: A tensor of floats containing loss evaluations at the\\n              previous 'n' points (as many points as the order) of the finite\\n              difference method.\\n            order: An integer indicating the order of\",\n",
       " \" the finite difference\\n              method we want to use. The function will use the length of the\\n              'input_array' array if no values is provided.\\n            verbose: Whether we want the function to print out information about\\n              c\",\n",
       " 'omputations or not.\\n\\n        Returns:\\n            The approximated derivative as a float value.\\n\\n        Raises:\\n            None.\\n\\n        \"\"\"\\n        return _get_finite_difference(input_array = input_tensor.numpy(),\\n                                      ',\n",
       " 'order = order,\\n                                      verbose = verbose)',\n",
       " 'def _get_finite_difference(input_array: numpy.array,\\n                           order: int = None,\\n                           verbose: bool = True):\\n    \"\"\"Internal utility method for estimating rate of change.\\n\\n    This function aims to approximate the ra',\n",
       " \"te of change for a loss function,\\n    which is used for the 'LossWeighted' and 'Normalized' variants of SoftAdapt.\\n\\n    For even accuracy orders, we take advantage of the `findiff` package\\n    (https://findiff.readthedocs.io/en/latest/source/examples-basic\",\n",
       " '.html).\\n    Accuracy orders of 1 (trivial), 3, and 5 are retrieved from an internal\\n    constants file. Due to the underlying mathematics of computing the\\n    coefficients, all accuracy orders higher than 5 must be an even number.\\n\\n    Args:\\n        input_',\n",
       " \"array: An array of floats containing loss evaluations at the\\n          previous 'n' points (as many points as the order) of the finite\\n          difference method.\\n        order: An integer indicating the order of the finite difference method\\n          we \",\n",
       " \"want to use. The function will use the length of the 'input_array'\\n          array if no values is provided.\\n        verbose: Whether we want the function to print out information about\\n          computations or not.\\n\\n    Returns:\\n        A float which is \",\n",
       " 'the approximated rate of change between the loss\\n        points.\\n\\n    Raises:\\n        ValueError: If the number of points in the `input_array` array is\\n          smaller than the order of accuracy we desire.\\n        Value Error: If the order of accuracy is',\n",
       " ' higher than 5 and it is not an\\n          even number.\\n    \"\"\"\\n    # First, we want to check the order and the number of loss points we are\\n    # given\\n    if order is None:\\n        order = len(input_array) - 1\\n        if verbose:\\n            print(f\"==> I',\n",
       " 'nterpreting finite difference order as {order} since\"\\n                  \"no explicit order was specified.\")\\n    else:\\n        if order > len(input_array):\\n            raise ValueError(\"The order of finite difference computations can\"\\n                      ',\n",
       " '       \"not be larger than the number of loss points. \"\\n                             \"Please check the order argument or wait until \"\\n                             \"enough points have been stored before calling the\"\\n                             \" method.\")\\n',\n",
       " '        elif order + 1 < len(input_array):\\n            print(f\"==> There are more points than \\'order\\' + 1 ({order + 1}) \"\\n                  f\"points (array contains {len(input_array)} values). Function\"\\n                  f\"will use the last {order} element',\n",
       " 's of loss points for \"\\n                  \"computations.\")\\n            input_array = input_array[(-1*order - 1):]\\n\\n    order_is_even = order % 2 == 0\\n    # Next, we want to retrieve the correct coefficients based on the order\\n    if order > 5 and not order_',\n",
       " 'is_even:\\n        raise ValueError(\"Accuracy orders larger than 5 must be even. Please \"\\n                         \"check the arguments passed to the function.\")\\n\\n    if order_is_even:\\n        constants = coefficients(deriv=1, acc=order)[\"forward\"][\"coeffici',\n",
       " 'ents\"]\\n\\n    else:\\n        if order == 1:\\n            constants = _FIRST_ORDER_COEFFICIENTS\\n        elif order == 3:\\n            constants = _THIRD_ORDER_COEFFICIENTS\\n        else:\\n            constants = _FIFTH_ORDER_COEFFICIENTS\\n\\n    pointwise_multiplicat',\n",
       " 'ion = [\\n        input_array[i] * constants[i] for i in range(len(constants))\\n    ]\\n    return numpy.sum(pointwise_multiplication)']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_code_snippets(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
